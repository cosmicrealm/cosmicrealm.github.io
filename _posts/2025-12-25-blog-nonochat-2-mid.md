---
title: 'noao-chat-2-midé˜¶æ®µè®­ç»ƒ'
date: 2025-12-25
permalink: /posts/2025/12/2025-12-25-blog-nonochat-2-mid/
tags:
  - llm
---


### nonochat - LLM Mid è®­ç»ƒå®Œæ•´è§£æ

[nonochat](https://github.com/karpathy/nanochat)


> æœ¬æ–‡æ¡£è¯¦ç»†è§£æ `mid_train.py`ï¼Œè®²è§£ä¸­é—´è®­ç»ƒï¼ˆMidtrainingï¼‰çš„ä½œç”¨å’Œå®ç°ç»†èŠ‚ã€‚  
> é€‚åˆ LLM é¢†åŸŸçš„åˆå­¦è€…ï¼Œä» Base æ¨¡å‹åˆ°åº”ç”¨æ¨¡å‹çš„å…³é”®è¿‡æ¸¡é˜¶æ®µã€‚

---

## ç›®å½•

1. [ä»€ä¹ˆæ˜¯ Mid è®­ç»ƒ](#1-ä»€ä¹ˆæ˜¯-mid-è®­ç»ƒ)
2. [æ•°æ®æ¥æºçš„å·¨å¤§å˜åŒ–](#2-æ•°æ®æ¥æºçš„å·¨å¤§å˜åŒ–)
3. [è®­ç»ƒæµç¨‹è¯¦è§£](#3-è®­ç»ƒæµç¨‹è¯¦è§£)
4. [ä¸ Base è®­ç»ƒçš„å¯¹æ¯”](#4-ä¸-base-è®­ç»ƒçš„å¯¹æ¯”)
5. [å®æˆ˜æ¡ˆä¾‹åˆ†æ](#5-å®æˆ˜æ¡ˆä¾‹åˆ†æ)

---

## 1. ä»€ä¹ˆæ˜¯ Mid è®­ç»ƒ

### 1.1 è®­ç»ƒé˜¶æ®µå®šä½

```
Base Training â†’ Mid Training â†’ SFT â†’ RL
     â†“              â†“            â†“     â†“
  é€šç”¨èƒ½åŠ›      é¢†åŸŸèƒ½åŠ›      å¯¹è¯   å¯¹é½
  (æµ·é‡æ–‡æœ¬)  (ç»“æ„åŒ–ä»»åŠ¡)   (æŒ‡ä»¤)  (åå¥½)
```

**Mid Trainingï¼ˆä¸­é—´è®­ç»ƒï¼‰** æ˜¯è¿æ¥é¢„è®­ç»ƒå’Œå¾®è°ƒçš„æ¡¥æ¢é˜¶æ®µã€‚

### 1.2 ä¸ºä»€ä¹ˆéœ€è¦ Mid Trainingï¼Ÿ

**Base æ¨¡å‹çš„å±€é™ï¼š**
- âœ… æœ‰é€šç”¨è¯­è¨€ç†è§£èƒ½åŠ›
- âŒ ä¸æ‡‚å¯¹è¯æ ¼å¼
- âŒ ä¸ä¼šä½¿ç”¨å·¥å…·
- âŒ ç¼ºä¹ç‰¹å®šé¢†åŸŸçŸ¥è¯†
- âŒ åœ¨æŸäº›ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³

**ç›´æ¥ SFT çš„é—®é¢˜ï¼š**
- SFT æ•°æ®é‡é€šå¸¸å¾ˆå°‘ï¼ˆå‡ åƒåˆ°å‡ ä¸‡æ¡ï¼‰
- å¦‚æœ Base æ¨¡å‹åœ¨æŸä¸ªèƒ½åŠ›ä¸Šå¾ˆå¼±ï¼ŒSFT å¾ˆéš¾è¡¥æ•‘
- æŸäº›åŸºç¡€èƒ½åŠ›éœ€è¦æ›´å¤šæ•°æ®æ‰èƒ½å­¦ä¼š

**Mid Training çš„è§£å†³æ–¹æ¡ˆï¼š**
- ğŸ¯ ä½¿ç”¨**ç»“æ„åŒ–å¯¹è¯æ•°æ®**ç»§ç»­è®­ç»ƒ
- ğŸ¯ æ³¨å…¥**ç‰¹å®šé¢†åŸŸçŸ¥è¯†**ï¼ˆæ•°å­¦ã€å¯¹è¯ã€å·¥å…·ä½¿ç”¨ï¼‰
- ğŸ¯ è¡¥é½ Base æ¨¡å‹çš„**èƒ½åŠ›çŸ­æ¿**
- ğŸ¯ ä¸ºåç»­ SFT æ‰“ä¸‹è‰¯å¥½åŸºç¡€

### 1.3 Mid Training çš„æ ¸å¿ƒç›®æ ‡

```python
# è®­ç»ƒæ•°æ®ç»„æˆï¼ˆæ€»å…± ~850K æ¡å¯¹è¯ï¼‰
train_dataset = TaskMixture([
    SmolTalk(split="train"),              # 460K é€šç”¨å¯¹è¯
    MMLU(subset="auxiliary_train"),        # 100K çŸ¥è¯†é—®ç­”
    GSM8K(subset="main", split="train"),   # 8K æ•°å­¦æ¨ç†
    CustomJSON(identity_file),             # 1K èº«ä»½å¯¹è¯ Ã— 2 è½®
    SimpleSpelling(size=200000),           # 200K æ‹¼å†™
    SpellingBee(size=80000),               # 80K å­—æ¯è®¡æ•°
])
```

**å…³é”®èƒ½åŠ›æ³¨å…¥ï¼š**

| èƒ½åŠ›ç±»å‹ | æ•°æ®æ¥æº | æ•°é‡ | ç›®çš„ |
|---------|---------|------|------|
| å¯¹è¯èƒ½åŠ› | SmolTalk | 460K | å­¦ä¼šè‡ªç„¶å¯¹è¯ |
| çŸ¥è¯†å‚¨å¤‡ | MMLU | 100K | å¤šé¢†åŸŸçŸ¥è¯† |
| æ•°å­¦æ¨ç† | GSM8K | 8K | å·¥å…·ä½¿ç”¨ + æ¨ç† |
| èº«ä»½è®¾å®š | è‡ªå®šä¹‰ | 1KÃ—2 | æ¨¡å‹äººæ ¼ |
| æ‹¼å†™èƒ½åŠ› | Spelling | 280K | tokenâ†’å­—ç¬¦æ˜ å°„ |

---

## 2. æ•°æ®æ¥æºçš„å·¨å¤§å˜åŒ–

### 2.1 Base vs Mid æ•°æ®å¯¹æ¯”

#### Base Training æ•°æ®æ ¼å¼

```
åŸå§‹æ–‡æœ¬ï¼ˆParquet æ–‡ä»¶ï¼‰:
"The quick brown fox jumps over the lazy dog. This is a test..."

Tokenize â†’ è¿ç»­çš„ token æµ:
[15496, 995, 831, 374, 264, 1296, ...]

åˆ†æ‰¹æ¬¡:
inputs:  [15496, 995, 831, 374, ...]
targets: [995, 831, 374, 264, ...]
```

**ç‰¹ç‚¹ï¼š**
- ğŸ“„ çº¯æ–‡æœ¬ï¼Œæ²¡æœ‰ç»“æ„
- ğŸ”„ æ–‡æ¡£è¾¹ç•Œè¢« `<|bos|>` åˆ†éš”
- ğŸ² éšæœºæ‹¼æ¥æ–‡æ¡£

#### Mid Training æ•°æ®æ ¼å¼

```
ç»“æ„åŒ–å¯¹è¯ï¼ˆæ¥è‡ªä»»åŠ¡æ•°æ®é›†ï¼‰:
{
    "messages": [
        {"role": "user", "content": "What is 2+2?"},
        {"role": "assistant", "content": "4"}
    ]
}

Tokenize â†’ å¸¦æ ¼å¼çš„ token åºåˆ—:
[<|bos|>, <|user_start|>, "What", "is", "2", "+", "2", "?", <|user_end|>, 
 <|assistant_start|>, "4", <|assistant_end|>]

åˆ†æ‰¹æ¬¡ï¼ˆä¿ç•™å¯¹è¯ç»“æ„ï¼‰:
# å¤šä¸ªå®Œæ•´å¯¹è¯æ‹¼æ¥åœ¨ä¸€èµ·
```

**ç‰¹ç‚¹ï¼š**
- ğŸ’¬ ç»“æ„åŒ–å¯¹è¯
- ğŸ·ï¸ æ˜ç¡®çš„è§’è‰²æ ‡è®°
- ğŸ“š æ¥è‡ªç²¾å¿ƒç­–åˆ’çš„ä»»åŠ¡

### 2.2 ä»»åŠ¡æ•°æ®é›†è¯¦è§£

#### 2.2.1 SmolTalk - é€šç”¨å¯¹è¯

**æ¥æºï¼š** HuggingFace SmolTalk æ•°æ®é›†

**æ ¼å¼ç¤ºä¾‹ï¼š**

```json
{
    "messages": [
        {"role": "user", "content": "Can you explain quantum computing?"},
        {"role": "assistant", "content": "Quantum computing uses quantum bits..."},
        {"role": "user", "content": "What are its applications?"},
        {"role": "assistant", "content": "Quantum computers can..."}
    ]
}
```

**ç‰¹ç‚¹ï¼š**
- å¤šè½®å¯¹è¯
- æ¶µç›–å„ç§æ—¥å¸¸è¯é¢˜
- è‡ªç„¶çš„å¯¹è¯é£æ ¼
- è®­ç»ƒé›†ï¼š460K æ¡ï¼Œæµ‹è¯•é›†ï¼š24K æ¡

**ä½œç”¨ï¼š** è®©æ¨¡å‹å­¦ä¼šè‡ªç„¶çš„å¤šè½®å¯¹è¯æ¨¡å¼

#### 2.2.2 MMLU - çŸ¥è¯†é—®ç­”

**æ¥æºï¼š** MMLU (Massive Multitask Language Understanding)

**æ ¼å¼ç¤ºä¾‹ï¼š**

```json
{
    "messages": [
        {
            "role": "user", 
            "content": "What is the capital of France?\nA. London\nB. Paris\nC. Berlin\nD. Madrid"
        },
        {"role": "assistant", "content": "B"}
    ],
    "subject": "geography",
    "letters": ["A", "B", "C", "D"]
}
```

**ç‰¹ç‚¹ï¼š**
- é€‰æ‹©é¢˜æ ¼å¼
- 57 ä¸ªå­¦ç§‘é¢†åŸŸ
- è¾…åŠ©è®­ç»ƒé›†ï¼š100K æ¡

**å­¦ç§‘åˆ†å¸ƒï¼š**
```
STEM: æ•°å­¦ã€ç‰©ç†ã€åŒ–å­¦ã€ç”Ÿç‰©ã€è®¡ç®—æœº...
äººæ–‡: å†å²ã€å“²å­¦ã€æ³•å¾‹ã€å¿ƒç†å­¦...
ç¤¾ç§‘: ç»æµå­¦ã€ç¤¾ä¼šå­¦ã€æ”¿æ²»å­¦...
å…¶ä»–: åŒ»å­¦ã€å•†ä¸šã€è‰ºæœ¯...
```

**ä½œç”¨ï¼š** æ³¨å…¥å¤§é‡ç»“æ„åŒ–çŸ¥è¯†

#### 2.2.3 GSM8K - æ•°å­¦æ¨ç†

**æ¥æºï¼š** GSM8K (Grade School Math 8K)

**æ ¼å¼ç¤ºä¾‹ï¼š**

```json
{
    "messages": [
        {
            "role": "user",
            "content": "Weng earns $12 an hour. Yesterday, she worked for 50 minutes. How much did she earn?"
        },
        {
            "role": "assistant",
            "content": [
                {"type": "text", "text": "First, let's calculate per minute wage:\n"},
                {"type": "python", "text": "12/60"},
                {"type": "python_output", "text": "0.2"},
                {"type": "text", "text": "\nSo she earns $0.2 per minute. For 50 minutes:\n"},
                {"type": "python", "text": "0.2*50"},
                {"type": "python_output", "text": "10"},
                {"type": "text", "text": "\n#### 10"}
            ]
        }
    ]
}
```

**å…³é”®ç‰¹æ€§ï¼š**
- ğŸ§® åŒ…å«å·¥å…·è°ƒç”¨ï¼ˆPython è®¡ç®—å™¨ï¼‰
- ğŸ“ æ­¥éª¤åŒ–æ¨ç†
- âœ… æœ€ç»ˆç­”æ¡ˆæ ‡è®° `#### 10`

**Token åŒ–åçš„æ ¼å¼ï¼š**

```
<|bos|>
<|user_start|> Weng earns $12 an hour... <|user_end|>
<|assistant_start|> 
    First, let's calculate per minute wage:
    <|python_start|> 12/60 <|python_end|>
    <|output_start|> 0.2 <|output_end|>
    So she earns $0.2 per minute. For 50 minutes:
    <|python_start|> 0.2*50 <|python_end|>
    <|output_start|> 10 <|output_end|>
    #### 10
<|assistant_end|>
```

**ä½œç”¨ï¼š** 
- æ•™ä¼šæ¨¡å‹ä½¿ç”¨å·¥å…·
- å­¦ä¹ æ­¥éª¤åŒ–æ¨ç†
- ç†è§£è®¡ç®—æµç¨‹

#### 2.2.4 Spelling Tasks - æ‹¼å†™èƒ½åŠ›

**ä¸ºä»€ä¹ˆéœ€è¦ï¼Ÿ**

LLM çš„ä¸€ä¸ªå¸¸è§å¼±ç‚¹ï¼šä¸çŸ¥é“ token æ˜¯å¦‚ä½•æ‹¼å†™çš„ã€‚

```
é—®é¢˜ç¤ºä¾‹ï¼š
"How many 'r' are in 'strawberry'?"

Base æ¨¡å‹å¯èƒ½å›ç­”: "2"  âŒ
æ­£ç¡®ç­”æ¡ˆ: "3"  âœ…
```

**åŸå› åˆ†æï¼š**

```
Token åŒ–:
"strawberry" â†’ [str, aw, berry]  # å¯èƒ½è¢«åˆ‡æˆå¤šä¸ª token

æ¨¡å‹çœ‹åˆ°çš„æ˜¯ token IDsï¼Œä¸æ˜¯å­—ç¬¦:
[24871, 675, 15717]

è¦å›ç­”"æœ‰å‡ ä¸ª r"ï¼Œæ¨¡å‹éœ€è¦çŸ¥é“:
- token 24871 åŒ…å«å“ªäº›å­—æ¯ï¼Ÿ
- token 675 åŒ…å«å“ªäº›å­—æ¯ï¼Ÿ
- token 15717 åŒ…å«å“ªäº›å­—æ¯ï¼Ÿ
```

è¿™ç§ **token â†’ å­—ç¬¦** çš„æ˜ å°„éœ€è¦ä¸“é—¨è®­ç»ƒï¼

**è§£å†³æ–¹æ¡ˆï¼šSimpleSpelling**

```json
{
    "messages": [
        {"role": "user", "content": "Spell the word 'apple'"},
        {"role": "assistant", "content": "a-p-p-l-e"}
    ]
}
```

- 200K ä¸ªå•è¯æ‹¼å†™ä»»åŠ¡
- è®©æ¨¡å‹å­¦ä¼š token çš„å­—ç¬¦ç»„æˆ

**è§£å†³æ–¹æ¡ˆï¼šSpellingBee**

```json
{
    "messages": [
        {"role": "user", "content": "How many r are in strawberry"},
        {
            "role": "assistant",
            "content": [
                {"type": "text", "text": "Let me spell it:\n"},
                {"type": "python", "text": "'strawberry'.lower()"},
                {"type": "python_output", "text": "strawberry"},
                {"type": "text", "text": "\nNow count:\n"},
                {"type": "python", "text": "'strawberry'.count('r')"},
                {"type": "python_output", "text": "3"},
                {"type": "text", "text": "\n#### 3"}
            ]
        }
    ]
}
```

- 80K ä¸ªå­—æ¯è®¡æ•°ä»»åŠ¡
- ç»“åˆæ‹¼å†™å’Œå·¥å…·ä½¿ç”¨

#### 2.2.5 Identity Conversations - èº«ä»½è®¾å®š

**æ ¼å¼ç¤ºä¾‹ï¼š**

```json
{
    "messages": [
        {"role": "user", "content": "Who are you?"},
        {"role": "assistant", "content": "I am an AI assistant created by..."}
    ]
}
```

**ä½œç”¨ï¼š**
- è®©æ¨¡å‹çŸ¥é“è‡ªå·±æ˜¯è°
- ç»Ÿä¸€å›ç­”èº«ä»½é—®é¢˜
- å¢å¼ºä¸€è‡´æ€§

### 2.3 æ•°æ®åŠ è½½å™¨å¯¹æ¯”

#### Base Training æ•°æ®åŠ è½½

```python
def tokenizing_distributed_data_loader(B, T, split):
    # è¯»å– Parquet æ–‡ä»¶
    pf = pq.ParquetFile(filepath)
    batch = rg.column('text').to_pylist()
    
    # Tokenize
    token_lists = tokenizer.encode(batch, prepend=bos_token)
    
    # æ‹¼æ¥æˆè¿ç»­æµ
    for tokens in token_lists:
        token_buffer.extend(tokens)
    
    # å–å‡ºå›ºå®šé•¿åº¦
    needed = B * T + 1
    tokens = [token_buffer.popleft() for _ in range(needed)]
    
    # åˆ‡åˆ†
    inputs = tokens[:-1].view(B, T)
    targets = tokens[1:].view(B, T)
```

#### Mid Training æ•°æ®åŠ è½½

```python
def mid_data_generator(split):
    dataset = train_dataset  # TaskMixture
    
    # éå†æ•°æ®é›†
    for cursor in range(ddp_rank, len(dataset), ddp_world_size):
        # è·å–ä¸€ä¸ªå¯¹è¯
        conversation = dataset[cursor]
        
        # Tokenizeï¼ˆä¿ç•™ç»“æ„ï¼‰
        ids, mask = tokenizer.render_conversation(conversation)
        # ids: [<|bos|>, <|user_start|>, ..., <|assistant_end|>]
        
        # ç´¯ç§¯åˆ°ç¼“å†²åŒº
        token_buffer.extend(ids)
        
        # å½“ç¼“å†²åŒºè¶³å¤Ÿæ—¶ï¼Œåˆ‡åˆ†æ‰¹æ¬¡
        if len(token_buffer) >= needed_tokens:
            tokens = [token_buffer.popleft() for _ in range(needed_tokens)]
            inputs = tokens[:-1].view(B, T)
            targets = tokens[1:].view(B, T)
            yield inputs, targets
```

**å…³é”®å·®å¼‚ï¼š**

| ç»´åº¦ | Base Training | Mid Training |
|-----|---------------|--------------|
| æ•°æ®æº | Parquet æ–‡ä»¶ï¼ˆçº¯æ–‡æœ¬ï¼‰ | Task å¯¹è±¡ï¼ˆç»“æ„åŒ–å¯¹è¯ï¼‰ |
| åˆ†è¯æ–¹å¼ | `encode(text)` | `render_conversation(conv)` |
| ç»“æ„ | æ— ç»“æ„ | å¸¦è§’è‰²æ ‡è®° |
| è¾¹ç•Œ | åªæœ‰ `<|bos|>` | å®Œæ•´çš„å¯¹è¯æ ‡è®° |
| å¯¹è¯å®Œæ•´æ€§ | æ–‡æ¡£å¯èƒ½è¢«æˆªæ–­ | å°½é‡ä¿æŒå¯¹è¯å®Œæ•´ |

---

## 3. è®­ç»ƒæµç¨‹è¯¦è§£

### 3.1 å®Œæ•´æ•°æ®æµï¼ˆå›¾è§£ï¼‰

```
æ­¥éª¤ 1: åŠ è½½ä»»åŠ¡æ•°æ®
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SmolTalk: 460K å¯¹è¯                  â”‚
â”‚ MMLU: 100K é—®ç­”                      â”‚
â”‚ GSM8K: 8K æ•°å­¦é¢˜                     â”‚
â”‚ Spelling: 280K æ‹¼å†™ä»»åŠ¡              â”‚
â”‚ Identity: 1K èº«ä»½å¯¹è¯                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
æ­¥éª¤ 2: TaskMixture æ··åˆ
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ‰€æœ‰ä»»åŠ¡æ··åˆæ‰“ä¹±                      â”‚
â”‚ æ€»è®¡: ~850K æ¡å¯¹è¯                   â”‚
â”‚ ç¡®å®šæ€§éšæœºæ‰“ä¹± (seed=42)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
æ­¥éª¤ 3: æŒ‰ç´¢å¼•éå†
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ¯ä¸ª GPU è´Ÿè´£ä¸åŒçš„å­é›†                â”‚
â”‚ GPU 0: indices [0, 8, 16, 24, ...]   â”‚
â”‚ GPU 1: indices [1, 9, 17, 25, ...]   â”‚
â”‚ ...                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
æ­¥éª¤ 4: è·å–å¯¹è¯
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ conversation = dataset[cursor]       â”‚
â”‚ {                                    â”‚
â”‚   "messages": [                      â”‚
â”‚     {"role": "user", "content": ...} â”‚
â”‚     {"role": "assistant", ...}       â”‚
â”‚   ]                                  â”‚
â”‚ }                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
æ­¥éª¤ 5: Tokenizeï¼ˆä¿ç•™ç»“æ„ï¼‰
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ids, mask = tokenizer.render_conversation(conv) â”‚
â”‚                                      â”‚
â”‚ ids: [<|bos|>, <|user_start|>, ...] â”‚
â”‚ mask: [0, 0, 0, 1, 1, ...]  # è®­ç»ƒæ©ç  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
æ­¥éª¤ 6: ç´¯ç§¯åˆ°ç¼“å†²åŒº
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ token_buffer.extend(ids)             â”‚
â”‚ [1, 2, 3, 4, ..., 10000, 10001, ...] â”‚
â”‚ ï¼ˆå¤šä¸ªå¯¹è¯è¿æ¥åœ¨ä¸€èµ·ï¼‰                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
æ­¥éª¤ 7: åˆ‡åˆ†æ‰¹æ¬¡
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ needed = B * T + 1  # 32*2048+1      â”‚
â”‚ tokens = buffer[:needed]             â”‚
â”‚                                      â”‚
â”‚ inputs:  (B, T) = (32, 2048)         â”‚
â”‚ targets: (B, T) = (32, 2048)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
æ­¥éª¤ 8: é€å…¥æ¨¡å‹è®­ç»ƒ
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ loss = model(inputs, targets)        â”‚
â”‚ loss.backward()                      â”‚
â”‚ optimizer.step()                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 TokenMixture çš„ä½œç”¨

**é—®é¢˜ï¼š** ä¸åŒä»»åŠ¡çš„æ•°æ®é‡å·®å¼‚å·¨å¤§

```
SmolTalk: 460K  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
MMLU:     100K  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
GSM8K:      8K  â–Œ
Identity:   2K  â–
```

**ç®€å•éå†çš„é—®é¢˜ï¼š**
```python
# é”™è¯¯åšæ³•ï¼šä¾æ¬¡è®­ç»ƒæ¯ä¸ªä»»åŠ¡
for task in tasks:
    for example in task:
        train(example)

# ç»“æœï¼šå‰æœŸéƒ½åœ¨è®­ç»ƒ SmolTalkï¼ŒåæœŸæ‰è§åˆ° GSM8K
# æ¨¡å‹ä¼šé—å¿˜æ—©æœŸå­¦åˆ°çš„ä¸œè¥¿ï¼
```

**TaskMixture çš„è§£å†³æ–¹æ¡ˆï¼š**

```python
class TaskMixture:
    def __init__(self, tasks):
        # 1. æ„å»ºç´¢å¼•æ˜ å°„
        self.index_map = []
        for task_idx, task in enumerate(tasks):
            for local_idx in range(len(task)):
                self.index_map.append((task_idx, local_idx))
        
        # 2. æ‰“ä¹±ç´¢å¼•ï¼ˆç¡®å®šæ€§ï¼‰
        rng = random.Random(42)
        rng.shuffle(self.index_map)
        
        # ç°åœ¨è®¿é—®é¡ºåºæ˜¯ï¼š
        # [SmolTalk_123, MMLU_45, GSM8K_7, SmolTalk_456, ...]
```

**æ•ˆæœï¼š**
```
è®­ç»ƒè¿‡ç¨‹ä¸­çœ‹åˆ°çš„æ•°æ®ï¼ˆæ··åˆåï¼‰ï¼š
[S, S, M, S, S, G, S, M, S, I, S, S, M, S, G, ...]
 â†‘  â†‘  â†‘  â†‘  â†‘  â†‘  â†‘  â†‘  â†‘  â†‘  â†‘  â†‘  â†‘  â†‘  â†‘
 SmolTalk, MMLU, GSM8K, Identity æ··åˆå‡ºç°

è€Œä¸æ˜¯ï¼š
[S, S, S, S, ..., M, M, M, ..., G, G, ...]
 â†â”€â”€ SmolTalk â”€â”€â”€â†’  â†MMLUâ†’  â†GSM8Kâ†’
```

**ä¼˜ç‚¹ï¼š**
- âœ… æ‰€æœ‰ä»»åŠ¡åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­éƒ½æœ‰æ›å…‰
- âœ… é¿å…ç¾éš¾æ€§é—å¿˜
- âœ… ä¸åŒèƒ½åŠ›åŒæ­¥æå‡

### 3.3 è®­ç»ƒå¾ªç¯è¯¦è§£

#### 3.3.1 è¿›åº¦è¿½è¸ªæœºåˆ¶

Mid Training çš„ä¸€ä¸ªç‰¹æ®Šä¹‹å¤„ï¼š**æˆ‘ä»¬ä¸æå‰çŸ¥é“è¦è®­ç»ƒå¤šå°‘æ­¥**

```python
# Base Training: æ˜ç¡®çš„è¿­ä»£æ¬¡æ•°
num_iterations = 10000  # ç¡®å®š
for step in range(num_iterations):
    ...

# Mid Training: åŸºäºæ•°æ®é›†å¤§å°
dataset_size = 850000  # å¯¹è¯æ•°é‡
# æ¯ä¸ª step æ¶ˆè€—å¤šå°‘å¯¹è¯ï¼Ÿä¸ç¡®å®šï¼ï¼ˆå› ä¸ºå¯¹è¯é•¿åº¦ä¸ä¸€ï¼‰
# æ‰€ä»¥ç”¨è¿›åº¦ç™¾åˆ†æ¯”
```

**è¿›åº¦è®¡ç®—ï¼š**

```python
# å…¨å±€å˜é‡
last_step = False       # æ˜¯å¦åˆ°è¾¾æœ€åä¸€æ­¥
approx_progress = 0.0   # 0 â†’ 1

# åœ¨æ•°æ®ç”Ÿæˆå™¨ä¸­æ›´æ–°
def mid_data_generator(split):
    cursor = ddp_rank
    it = 0
    
    while True:
        # å¤„ç†ä¸€ä¸ªå¯¹è¯
        conversation = dataset[cursor]
        cursor += ddp_world_size
        
        # æ›´æ–°è¿›åº¦
        if cursor >= dataset_size:
            cursor -= dataset_size  # å›ç»•ï¼ˆä¸‹ä¸€ä¸ª epochï¼‰
            if split == "train":
                last_step = True  # æ ‡è®°ä¸ºæœ€åä¸€æ­¥
        
        # è®¡ç®—è¿‘ä¼¼è¿›åº¦
        approx_progress = cursor / dataset_size
        
        yield inputs, targets
```

**å¤šå¡åŒæ­¥ï¼š**

```python
# é—®é¢˜ï¼šä¸åŒ GPU å¯èƒ½å¤„ç†é€Ÿåº¦ä¸åŒ
# GPU 0: last_step = True
# GPU 1: last_step = False  â† è¿˜æ²¡å®Œæˆ

# è§£å†³ï¼šåŒæ­¥ last_step
if ddp:
    last_step_tensor = torch.tensor(last_step, device=device)
    dist.all_reduce(last_step_tensor, op=dist.ReduceOp.MAX)
    # åªè¦æœ‰ä¸€ä¸ª GPU åˆ°è¾¾ç»ˆç‚¹ï¼Œæ‰€æœ‰ GPU éƒ½åœæ­¢
    last_step = bool(last_step_tensor.item())
```

#### 3.3.2 å­¦ä¹ ç‡è°ƒåº¦

**ä¸ Base Training ä¸åŒï¼š**

```python
# Base Training: åŸºäºæ­¥æ•°
def get_lr_multiplier(step):
    if step < warmup_iters:
        return step / warmup_iters
    elif step <= num_iterations - warmdown_iters:
        return 1.0
    else:
        return linear_decay(...)

# Mid Training: åŸºäºè¿›åº¦ç™¾åˆ†æ¯”
def get_lr_multiplier(progress):
    # progress: 0.0 â†’ 1.0
    if progress < 0.8:
        return 1.0  # å‰ 80% ä¿æŒä¸å˜
    else:
        # å 20% çº¿æ€§è¡°å‡åˆ° 0
        return 1 - (progress - 0.8) / 0.2
```

**å­¦ä¹ ç‡æ›²çº¿ï¼š**

```
LR Multiplier
    1.0 |â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•²
        |                                 â•²
        |                                  â•²
    0.5 |                                   â•²
        |                                    â•²
    0.0 |                                     â•
        +â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
        0%      20%     40%     60%     80%   100% progress
        â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ä¿æŒ 1.0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â† è¡°å‡ â†’
```

**ä¸ºä»€ä¹ˆä¸åŒï¼Ÿ**
- Base Training çŸ¥é“æ€»æ­¥æ•°ï¼Œå¯ä»¥ç²¾ç¡®è§„åˆ’
- Mid Training ä¸ç¡®å®šæ€»æ­¥æ•°ï¼Œç”¨è¿›åº¦ç™¾åˆ†æ¯”æ›´çµæ´»

#### 3.3.3 å•æ­¥è®­ç»ƒæµç¨‹

```python
# é¢„å–ç¬¬ä¸€æ‰¹æ•°æ®
x, y = next(train_loader)

while True:
    # === è¯„ä¼°é˜¶æ®µ ===
    if step % eval_every == 0:
        model.eval()
        val_bpb = evaluate_bpb(...)
        model.train()
    
    # === ä¿å­˜æ£€æŸ¥ç‚¹ ===
    if last_step:
        save_checkpoint(...)
        break
    
    # === è®­ç»ƒé˜¶æ®µ ===
    synchronize()
    t0 = time.time()
    
    # æ¢¯åº¦ç´¯ç§¯
    for micro_step in range(grad_accum_steps):
        with autocast_ctx:
            loss = model(x, y)
        
        loss = loss / grad_accum_steps
        loss.backward()
        
        # å¼‚æ­¥é¢„å–ä¸‹ä¸€æ‰¹
        x, y = next(train_loader)
        progress = max(progress, approx_progress)
    
    # æ›´æ–°å­¦ä¹ ç‡
    lrm = get_lr_multiplier(progress)
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * lrm
    
    # æ›´æ–°å‚æ•°
    for opt in optimizers:
        opt.step()
    model.zero_grad()
    
    synchronize()
    t1 = time.time()
    
    # === æ—¥å¿—è®°å½• ===
    step += 1
    dt = t1 - t0
    print(f"step {step} ({progress*100:.2f}%) | loss: {loss:.6f} ...")
```

### 3.4 ç»´åº¦åˆ†æ

å‡è®¾é…ç½®ï¼š`B=32, T=2048, vocab_size=32000, n_embd=768`

#### å•ä¸ªå¯¹è¯çš„ Token åŒ–

```python
# è¾“å…¥ï¼šç»“æ„åŒ–å¯¹è¯
conversation = {
    "messages": [
        {"role": "user", "content": "What is 2+2?"},
        {"role": "assistant", "content": "4"}
    ]
}

# è¾“å‡ºï¼štoken IDs + mask
ids, mask = tokenizer.render_conversation(conversation)

# ids: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
# mask: [0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0]

# å±•å¼€çœ‹ï¼š
# token_id | token_str           | mask | è¯´æ˜
# ---------|---------------------|------|------
# 1        | <|bos|>             | 0    | æ–‡æ¡£å¼€å§‹
# 2        | <|user_start|>      | 0    | ç”¨æˆ·å¼€å§‹
# 3-7      | "What is 2+2?"      | 0    | ç”¨æˆ·æ¶ˆæ¯ï¼ˆä¸è®­ç»ƒï¼‰
# 8        | <|user_end|>        | 0    | ç”¨æˆ·ç»“æŸ
# 9        | <|assistant_start|> | 0    | åŠ©æ‰‹å¼€å§‹
# 10-11    | "4"                 | 1    | åŠ©æ‰‹å›å¤ï¼ˆè®­ç»ƒï¼ï¼‰
# 12       | <|assistant_end|>   | 1    | åŠ©æ‰‹ç»“æŸï¼ˆè®­ç»ƒï¼‰
```

#### æ‰¹æ¬¡ç»´åº¦

```python
# å¤šä¸ªå¯¹è¯æ‹¼æ¥
conversation_1: 200 tokens
conversation_2: 150 tokens
conversation_3: 180 tokens
...

# ç´¯ç§¯åˆ°ç¼“å†²åŒº
token_buffer: [tok1, tok2, ..., tok_N]  # N å¾ˆå¤§

# åˆ‡åˆ†æ‰¹æ¬¡
needed = 32 * 2048 + 1 = 65537

tokens = token_buffer[:65537]
# [tok_1, tok_2, ..., tok_65537]

# é‡å¡‘
inputs = tokens[:-1].view(32, 2048)   # (32, 2048)
targets = tokens[1:].view(32, 2048)   # (32, 2048)
```

#### æ¨¡å‹å‰å‘ä¼ æ’­

```python
# è¾“å…¥
inputs: (32, 2048)  # int32

# Embedding
x = model.transformer.wte(inputs)
# (32, 2048) â†’ (32, 2048, 768)

# Transformer Blocks
for block in model.transformer.h:
    x = block(x, ...)
# (32, 2048, 768) â†’ (32, 2048, 768)

# LM Head
logits = model.lm_head(x)
# (32, 2048, 768) @ (768, 32000) â†’ (32, 2048, 32000)

# æŸå¤±è®¡ç®—
loss = F.cross_entropy(
    logits.view(-1, 32000),   # (65536, 32000)
    targets.view(-1)          # (65536,)
)
# loss: æ ‡é‡
```

---

## 4. ä¸ Base è®­ç»ƒçš„å¯¹æ¯”

### 4.1 æ ¸å¿ƒå·®å¼‚æ€»ç»“

| ç»´åº¦ | Base Training | Mid Training |
|-----|---------------|--------------|
| **æ•°æ®æ¥æº** | Parquet æ–‡ä»¶ï¼ˆç½‘é¡µæŠ“å–ï¼‰ | ä»»åŠ¡æ•°æ®é›†ï¼ˆç²¾å¿ƒç­–åˆ’ï¼‰ |
| **æ•°æ®æ ¼å¼** | çº¯æ–‡æœ¬ | ç»“æ„åŒ–å¯¹è¯ |
| **æ•°æ®é‡** | æ•°åäº¿ tokens | æ•°ç™¾ä¸‡å¯¹è¯ |
| **è®­ç»ƒç›®æ ‡** | é€šç”¨è¯­è¨€ç†è§£ | ç‰¹å®šèƒ½åŠ›æ³¨å…¥ |
| **Token æ ¼å¼** | è¿ç»­æ–‡æœ¬æµ | å¸¦è§’è‰²æ ‡è®° |
| **è¿›åº¦è¿½è¸ª** | å›ºå®šæ­¥æ•° | æ•°æ®é›†è¿›åº¦ |
| **å­¦ä¹ ç‡** | åŸºäºæ­¥æ•°è°ƒåº¦ | åŸºäºè¿›åº¦ç™¾åˆ†æ¯” |
| **è®­ç»ƒæ—¶é•¿** | æ•°å¤©åˆ°æ•°å‘¨ | æ•°å°æ—¶åˆ°1å¤© |
| **æ£€æŸ¥ç‚¹ä½ç½®** | `base_checkpoints/` | `mid_checkpoints/` |

### 4.2 ä»£ç å±‚é¢å¯¹æ¯”

#### æ•°æ®åŠ è½½å™¨

```python
# ===== Base Training =====
# ä» Parquet è¯»å–
pf = pq.ParquetFile(filepath)
batch = rg.column('text').to_pylist()

# ç®€å• tokenize
token_lists = tokenizer.encode(batch, prepend=bos_token)

# æ— ç»“æ„ï¼Œç›´æ¥æ‹¼æ¥
for tokens in token_lists:
    token_buffer.extend(tokens)

# ===== Mid Training =====
# ä» Task å¯¹è±¡è¯»å–
conversation = dataset[cursor]

# ç»“æ„åŒ– tokenize
ids, mask = tokenizer.render_conversation(conversation)
# åŒ…å« <|user_start|>, <|assistant_start|> ç­‰ç‰¹æ®Š token

# ä¿ç•™å¯¹è¯ç»“æ„
token_buffer.extend(ids)
```

#### æ¨¡å‹åˆå§‹åŒ–

```python
# ===== Base Training =====
# ä»é›¶å¼€å§‹
with torch.device("meta"):
    model_config = GPTConfig(...)
    model = GPT(model_config)
model.to_empty(device=device)
model.init_weights()  # éšæœºåˆå§‹åŒ–

# ===== Mid Training =====
# ä» Base æ¨¡å‹åŠ è½½
model, tokenizer, meta = load_model(
    "base",  # åŠ è½½ base æ¨¡å‹
    device,
    phase="train",
    model_tag=model_tag,
    step=step
)
# å‚æ•°å·²ç»è®­ç»ƒè¿‡ï¼Œç»§ç»­è®­ç»ƒ
```

#### è®­ç»ƒå¾ªç¯

```python
# ===== Base Training =====
for step in range(num_iterations):  # å›ºå®šæ¬¡æ•°
    loss = model(x, y)
    loss.backward()
    optimizer.step()

# ===== Mid Training =====
while True:  # ç›´åˆ°éå†å®Œæ•°æ®é›†
    loss = model(x, y)
    loss.backward()
    optimizer.step()
    
    if last_step:  # æ•°æ®é›†éå†å®Œ
        break
```

### 4.3 è¶…å‚æ•°å¯¹æ¯”

```python
# ===== Base Training =====
base_config = {
    "total_batch_size": 524288,      # 512K tokens/step
    "device_batch_size": 32,
    "num_iterations": 50000,         # æ˜ç¡®æŒ‡å®š
    "warmup_ratio": 0.0,             # æ—  warmup
    "warmdown_ratio": 0.2,           # æœ€å 20% è¡°å‡
    "embedding_lr": 0.2,
    "matrix_lr": 0.02,
    "unembedding_lr": 0.004,
    "init_lr_frac": 1.0,             # ä» 100% å¼€å§‹
}

# ===== Mid Training =====
mid_config = {
    "total_batch_size": 524288,      # 512K tokens/step (ç›¸åŒ)
    "device_batch_size": 32,
    "num_iterations": -1,            # è‡ªåŠ¨æ¨æ–­
    "warmup_ratio": 0.0,             # æ—  warmup
    "warmdown_ratio": 0.2,           # æœ€å 20% è¡°å‡
    "embedding_lr": 0.2,
    "matrix_lr": 0.02,
    "unembedding_lr": 0.004,
    "init_lr_frac": 1.0,             # ä» 100% å¼€å§‹
}
```

**ç›¸åŒç‚¹ï¼š**
- æ‰¹æ¬¡å¤§å°ç›¸åŒ
- å­¦ä¹ ç‡ç›¸åŒ
- ä¼˜åŒ–å™¨é…ç½®ç›¸åŒ

**ä¸åŒç‚¹ï¼š**
- Mid Training ä»å·²è®­ç»ƒçš„æ¨¡å‹å¼€å§‹
- æ•°æ®æºå®Œå…¨ä¸åŒ
- è®­ç»ƒç›®æ ‡ä¸åŒ

---

## 5. å®æˆ˜æ¡ˆä¾‹åˆ†æ

### 5.1 å®Œæ•´è®­ç»ƒæµç¨‹ç¤ºä¾‹

å‡è®¾æˆ‘ä»¬è®­ç»ƒä¸€ä¸ª `depth=12` çš„æ¨¡å‹ï¼ˆçº¦ 200M å‚æ•°ï¼‰ã€‚

#### æ­¥éª¤ 1: Base Training

```bash
# 8 å¡ A100ï¼Œè®­ç»ƒ 3 å¤©
torchrun --nproc_per_node=8 -m scripts.base_train \
    --depth=12 \
    --device_batch_size=32 \
    --total_batch_size=524288 \
    --num_iterations=50000 \
    --run=base_d12

# è¾“å‡ºï¼š
# base_checkpoints/d12/step_00050000.pt
```

**ç»“æœï¼š**
- âœ… æ¨¡å‹å­¦ä¼šäº†é€šç”¨è¯­è¨€ç†è§£
- âœ… èƒ½å¤Ÿè¡¥å…¨å¥å­
- âŒ ä¸æ‡‚å¯¹è¯æ ¼å¼
- âŒ ä¸ä¼šä½¿ç”¨å·¥å…·

#### æ­¥éª¤ 2: Mid Training

```bash
# 8 å¡ A100ï¼Œè®­ç»ƒ 6 å°æ—¶
torchrun --nproc_per_node=8 -m scripts.mid_train \
    --device_batch_size=32 \
    --total_batch_size=524288 \
    --run=mid_d12

# è¾“å‡ºï¼š
# mid_checkpoints/d12/step_XXXXX.pt
```

**ç»“æœï¼š**
- âœ… ç†è§£å¯¹è¯æ ¼å¼ï¼ˆuser/assistantï¼‰
- âœ… å­¦ä¼šäº†åŸºæœ¬å¯¹è¯èƒ½åŠ›
- âœ… èƒ½ä½¿ç”¨ Python è®¡ç®—å™¨
- âœ… æ‹¼å†™èƒ½åŠ›æ˜¾è‘—æå‡
- âœ… å¤šé¢†åŸŸçŸ¥è¯†å¢å¼º
- âŒ è¿˜ä¸èƒ½å¾ˆå¥½åœ°éµå¾ªæŒ‡ä»¤

### 5.2 è®­ç»ƒæ•°æ®æµè½¬ç¤ºä¾‹

è®©æˆ‘ä»¬è¿½è¸ªä¸€ä¸ªå®é™…çš„è®­ç»ƒ batchï¼š

#### Batch æ„æˆï¼ˆB=4, T=512 ä¸ºä¾‹ï¼‰

```python
# å‡è®¾ token_buffer ä¸­æœ‰ä»¥ä¸‹å¯¹è¯ï¼ˆç®€åŒ–ï¼‰ï¼š

# å¯¹è¯ 1: SmolTalkï¼ˆ200 tokensï¼‰
[<|bos|>, <|user_start|>, "How", "are", "you", "?", <|user_end|>,
 <|assistant_start|>, "I'm", "doing", "well", "!", <|assistant_end|>, ...]

# å¯¹è¯ 2: MMLUï¼ˆ50 tokensï¼‰
[<|bos|>, <|user_start|>, "What", "is", "2+2", "?", "\n", "A.", "2", ...,
 <|user_end|>, <|assistant_start|>, "D", <|assistant_end|>]

# å¯¹è¯ 3: GSM8Kï¼ˆ300 tokensï¼‰
[<|bos|>, <|user_start|>, "Weng", "earns", ..., <|user_end|>,
 <|assistant_start|>, "First", ",", "let's", ..., <|python_start|>, "12/60", ...]

# å¯¹è¯ 4: SpellingBeeï¼ˆ100 tokensï¼‰
[<|bos|>, <|user_start|>, "How", "many", "r", "in", "strawberry", <|user_end|>,
 <|assistant_start|>, <|python_start|>, "'strawberry'.count('r')", ...]

# ä» buffer ä¸­å–å‡º 4*512+1 = 2049 ä¸ª token
# è¿™äº› token æ¥è‡ªä¸Šè¿°å¯¹è¯çš„æ··åˆ
```

#### åˆ‡åˆ†æˆ inputs å’Œ targets

```python
tokens = token_buffer[:2049]

inputs = tokens[:-1]   # å‰ 2048 ä¸ª
targets = tokens[1:]   # å 2048 ä¸ª

# é‡å¡‘
inputs = inputs.view(4, 512)
targets = targets.view(4, 512)

# ç°åœ¨æ¯ä¸ªæ ·æœ¬åŒ…å«å¤šä¸ªå¯¹è¯çš„ç‰‡æ®µ
# æ ·æœ¬ 0: å¯¹è¯1çš„ä¸€éƒ¨åˆ† + å¯¹è¯2çš„å¼€å¤´
# æ ·æœ¬ 1: å¯¹è¯2çš„å‰©ä½™ + å¯¹è¯3çš„ä¸€éƒ¨åˆ†
# æ ·æœ¬ 2: å¯¹è¯3çš„ä¸­é—´éƒ¨åˆ†
# æ ·æœ¬ 3: å¯¹è¯3çš„ç»“å°¾ + å¯¹è¯4
```

#### æ¨¡å‹è®­ç»ƒ

```python
# å‰å‘ä¼ æ’­
logits = model(inputs)  # (4, 512, 32000)

# è®¡ç®—æŸå¤±
loss = F.cross_entropy(
    logits.view(-1, 32000),
    targets.view(-1)
)

# åå‘ä¼ æ’­
loss.backward()

# æ›´æ–°å‚æ•°
optimizer.step()
```

### 5.3 è®­ç»ƒæ—¥å¿—è§£è¯»

```
step 00100 (12.50%) | loss: 2.345678 | lrm: 1.00 | dt: 245.32ms | 
tok/sec: 2,137,856 | mfu: 45.67 | total time: 8.52m

step 00200 (25.00%) | loss: 2.123456 | lrm: 1.00 | dt: 243.11ms | 
tok/sec: 2,156,234 | mfu: 46.12 | total time: 17.21m

...

step 01600 (80.00%) | loss: 1.876543 | lrm: 1.00 | dt: 242.55ms | 
tok/sec: 2,161,345 | mfu: 46.23 | total time: 137.45m

step 01700 (85.00%) | loss: 1.865432 | lrm: 0.75 | dt: 242.88ms | 
tok/sec: 2,159,876 | mfu: 46.19 | total time: 145.78m
                                      ^^^^
                                      å­¦ä¹ ç‡å¼€å§‹è¡°å‡

step 02000 (100.00%) | loss: 1.854321 | lrm: 0.00 | dt: 243.21ms | 
tok/sec: 2,157,234 | mfu: 46.14 | total time: 170.12m
```

**è§‚å¯Ÿï¼š**
- è¿›åº¦ä» 0% â†’ 100%
- Loss ä» 2.3 â†’ 1.8ï¼ˆæ˜¾è‘—ä¸‹é™ï¼‰
- åœ¨ 80% ä¹‹åï¼Œlrm ä» 1.0 å¼€å§‹è¡°å‡
- æ€»è®­ç»ƒæ—¶é—´çº¦ 2.8 å°æ—¶ï¼ˆ8 å¡ï¼‰

### 5.4 èƒ½åŠ›æå‡å¯¹æ¯”

**æµ‹è¯•ä»»åŠ¡ï¼š**

```python
prompts = [
    # 1. å¯¹è¯èƒ½åŠ›
    "Hello! How are you?",
    
    # 2. çŸ¥è¯†é—®ç­”
    "What is the capital of France?",
    
    # 3. æ•°å­¦æ¨ç†
    "If I have 5 apples and buy 3 more, how many do I have?",
    
    # 4. æ‹¼å†™èƒ½åŠ›
    "How many 'r' are in 'strawberry'?",
]
```

**æ¨¡å‹å¯¹æ¯”ï¼š**

| æµ‹è¯• | Base æ¨¡å‹ | Mid æ¨¡å‹ |
|-----|----------|---------|
| å¯¹è¯ | "I are you" (è¯­æ³•é”™è¯¯) | "I'm doing well, thanks! How about you?" âœ… |
| çŸ¥è¯† | "Paris France city" (ä¸è¿è´¯) | "Paris" âœ… |
| æ•°å­¦ | "8 apples total" (ç›´æ¥çŒœ) | "5 + 3 = 8" âœ… (ä½¿ç”¨è®¡ç®—) |
| æ‹¼å†™ | "2" âŒ | "3" âœ… |

---

## 6. å…³é”®è¦ç‚¹æ€»ç»“

### 6.1 Mid Training çš„æœ¬è´¨

```
Mid Training = Base Model + Structured Tasks
             = é€šç”¨èƒ½åŠ› + ç‰¹å®šæŠ€èƒ½
             = ä¸º SFT åšå‡†å¤‡
```

### 6.2 ä½•æ—¶éœ€è¦ Mid Trainingï¼Ÿ

**éœ€è¦ Mid Trainingï¼š**
- âœ… Base æ¨¡å‹åœ¨æŸäº›èƒ½åŠ›ä¸Šå¾ˆå¼±ï¼ˆå¦‚æ‹¼å†™ã€å·¥å…·ä½¿ç”¨ï¼‰
- âœ… æœ‰å¤§é‡ç»“æ„åŒ–ä»»åŠ¡æ•°æ®
- âœ… æƒ³è¦æ³¨å…¥ç‰¹å®šé¢†åŸŸçŸ¥è¯†
- âœ… æ•°æ®é‡å¤Ÿå¤§ï¼ˆæ•°åä¸‡çº§åˆ«ï¼‰

**å¯ä»¥è·³è¿‡ Mid Trainingï¼š**
- âŒ Base æ¨¡å‹å·²ç»è¶³å¤Ÿå¼º
- âŒ åªæœ‰å°‘é‡ SFT æ•°æ®
- âŒ èµ„æºæœ‰é™

### 6.3 Mid Training vs SFT

| ç»´åº¦ | Mid Training | SFT |
|-----|-------------|-----|
| æ•°æ®é‡ | å¤§ï¼ˆæ•°åä¸‡ï¼‰ | å°ï¼ˆæ•°åƒï¼‰ |
| ç›®æ ‡ | èƒ½åŠ›æ‰©å±• | æŒ‡ä»¤å¯¹é½ |
| å­¦ä¹ ç‡ | è¾ƒé«˜ | è¾ƒä½ |
| è®­ç»ƒè½®æ•° | 1 epoch | 1-3 epochs |
| æ•°æ®æ¥æº | å…¬å¼€æ•°æ®é›† | ç²¾å¿ƒæ ‡æ³¨ |

### 6.4 å®è·µå»ºè®®

**æ•°æ®é…æ¯”ï¼š**
```python
# æ¨èé…æ¯”
train_dataset = TaskMixture([
    SmolTalk(...),          # 50-60% é€šç”¨å¯¹è¯
    Knowledge_QA(...),      # 15-20% çŸ¥è¯†é—®ç­”
    Math_Reasoning(...),    # 1-5% æ•°å­¦æ¨ç†
    Tool_Use(...),          # 1-5% å·¥å…·ä½¿ç”¨
    Spelling(...),          # 20-30% æ‹¼å†™/åŸºç¡€èƒ½åŠ›
    Identity(...),          # <1% èº«ä»½è®¾å®šï¼ˆè¿‡é‡‡æ ·ï¼‰
])
```

**è®­ç»ƒæ—¶é•¿ï¼š**
- å°æ¨¡å‹ï¼ˆ<500Mï¼‰ï¼š1-2 å°æ—¶ï¼ˆ8å¡ï¼‰
- ä¸­æ¨¡å‹ï¼ˆ500M-2Bï¼‰ï¼š4-8 å°æ—¶ï¼ˆ8å¡ï¼‰
- å¤§æ¨¡å‹ï¼ˆ>2Bï¼‰ï¼š1-2 å¤©ï¼ˆ8å¡ï¼‰

**æ£€æŸ¥ç‚¹ä¿å­˜ï¼š**
- åªä¿å­˜æœ€åçš„æ£€æŸ¥ç‚¹
- Mid Training é€šå¸¸ä¸éœ€è¦ä¸­é—´æ£€æŸ¥ç‚¹

---

## é™„å½•

### A. å®Œæ•´è®­ç»ƒå‘½ä»¤

```bash
# å• GPU è®­ç»ƒ
python -m scripts.mid_train \
    --device_batch_size=16 \
    --run=mid_test

# å¤š GPU è®­ç»ƒ
torchrun --standalone --nproc_per_node=8 \
    -m scripts.mid_train \
    --device_batch_size=32 \
    --total_batch_size=524288 \
    --run=mid_production
```

### B. ä»»åŠ¡æ•°æ®é›†è¯¦ç»†ç»Ÿè®¡

| ä»»åŠ¡ | è®­ç»ƒé›† | æµ‹è¯•é›† | å¹³å‡é•¿åº¦ | ç”¨é€” |
|-----|-------|-------|----------|------|
| SmolTalk | 460K | 24K | ~150 tokens | å¯¹è¯èƒ½åŠ› |
| MMLU (aux) | 100K | 14K | ~100 tokens | çŸ¥è¯†é—®ç­” |
| GSM8K | 8K | 1.3K | ~250 tokens | æ•°å­¦æ¨ç† |
| SimpleSpelling | 200K | - | ~30 tokens | æ‹¼å†™åŸºç¡€ |
| SpellingBee | 80K | - | ~100 tokens | å­—æ¯è®¡æ•° |
| Identity | 1K | - | ~80 tokens | èº«ä»½è®¾å®š |

### C. å¸¸è§é—®é¢˜

**Q1: Mid Training å¿…é¡»åšå—ï¼Ÿ**
- ä¸æ˜¯å¿…é¡»ï¼Œä½†å¼ºçƒˆæ¨è
- ç‰¹åˆ«æ˜¯æ¨¡å‹è¾ƒå°ï¼ˆ<1Bï¼‰æ—¶

**Q2: å¯ä»¥åšå¤šè½® Mid Training å—ï¼Ÿ**
- å¯ä»¥ï¼Œä½†é€šå¸¸ 1 è½®å°±å¤Ÿ
- æ›´å¤šè½®å¯èƒ½è¿‡æ‹Ÿåˆ

**Q3: å¦‚ä½•é€‰æ‹©ä»»åŠ¡ï¼Ÿ**
- æ ¹æ®åº”ç”¨åœºæ™¯é€‰æ‹©
- ä¿è¯æ•°æ®è´¨é‡
- æ³¨æ„ä»»åŠ¡é—´çš„å¹³è¡¡

**Q4: Mid Training å loss åº”è¯¥æ˜¯å¤šå°‘ï¼Ÿ**
- é€šå¸¸æ¯” Base Training ç•¥ä½
- å…¸å‹å€¼ï¼š1.5 - 2.0
- é‡è¦çš„æ˜¯ç›¸å¯¹ä¸‹é™ï¼Œä¸æ˜¯ç»å¯¹å€¼

---

## 9. å…³é”®ç–‘é—®è§£ç­”ï¼šä¸ºä»€ä¹ˆ Mid è®­ç»ƒä¸¢å¼ƒäº† Maskï¼Ÿ

### 9.1 Mask æœºåˆ¶çš„æœ¬è´¨

åœ¨ tokenizer ä¸­ï¼Œ`render_conversation` å‡½æ•°ä¼šè¿”å›ä¸¤ä¸ªå€¼ï¼š

```python
ids, mask = tokenizer.render_conversation(conversation)
# ids: [token_id1, token_id2, ...]  å®Œæ•´çš„ token åºåˆ—
# mask: [0, 0, 1, 1, 0, ...]        æ ‡è®°å“ªäº› token éœ€è¦è®­ç»ƒ
```

**Mask çš„å«ä¹‰**ï¼š
- `mask = 0`ï¼šä¸è®¡ç®— lossï¼ˆå¦‚ user çš„è¾“å…¥ã€ç‰¹æ®Š tokenï¼‰
- `mask = 1`ï¼šè®¡ç®— lossï¼ˆå¦‚ assistant çš„å›å¤ï¼‰

**Mask çš„ç”Ÿæˆé€»è¾‘**ï¼ˆæ¥è‡ª tokenizer.pyï¼‰ï¼š

```python
def render_conversation(self, conversation):
    ids, mask = [], []
    
    def add_tokens(token_ids, mask_val):
        ids.extend(token_ids)
        mask.extend([mask_val] * len(token_ids))
    
    # BOS token: mask=0
    add_tokens(bos, 0)
    
    for message in messages:
        if message["role"] == "user":
            # User çš„æ‰€æœ‰å†…å®¹ï¼šmask=0
            add_tokens(user_start, 0)
            add_tokens(content_ids, 0)  # â† User è¾“å…¥ä¸è®­ç»ƒ
            add_tokens(user_end, 0)
        
        elif message["role"] == "assistant":
            add_tokens(assistant_start, 0)  # Start token ä¸è®­ç»ƒ
            add_tokens(content_ids, 1)      # â† Assistant å›å¤è®­ç»ƒï¼
            add_tokens(assistant_end, 1)    # End token ä¹Ÿè®­ç»ƒ
    
    return ids, mask
```

### 9.2 Mid Training ä¸¢å¼ƒ Mask çš„çœŸç›¸

**ä»£ç å¯¹æ¯”**ï¼š

```python
# Mid Training (mid_train.py ç¬¬ 133 è¡Œ)
ids, _ = tokenizer.render_conversation(conversation)
#      â†‘ ç›´æ¥ä¸¢å¼ƒ maskï¼

# SFT Training (chat_sft.py ç¬¬ 123 è¡Œ)
ids, mask = tokenizer.render_conversation(doc)
#      â†‘ ä¿ç•™å¹¶ä½¿ç”¨ mask
```

**ä¸ºä»€ä¹ˆ Mid Training ä¸¢å¼ƒ maskï¼Ÿ**

ç­”æ¡ˆï¼š**Mid Training åœ¨æ‰€æœ‰ token ä¸Šè®¡ç®— lossï¼ŒåŒ…æ‹¬ User çš„è¾“å…¥ï¼**

### 9.3 Mid vs SFTï¼šè®­ç»ƒç›®æ ‡çš„æ ¹æœ¬åŒºåˆ«

#### Mid Training çš„ç›®æ ‡ï¼šå­¦ä¹ å¯¹è¯æµ

```python
# Mid Training çš„æ•°æ®å¤„ç†
conversation = dataset[cursor]
ids, _ = tokenizer.render_conversation(conversation)
token_buffer.extend(ids)  # ç›´æ¥æ‹¼æ¥æ‰€æœ‰ token

# ç”Ÿæˆ inputs, targetsï¼ˆæ²¡æœ‰ maskï¼‰
inputs = tokens[:-1]   # (B, T)
targets = tokens[1:]   # (B, T)

# è®¡ç®— lossï¼ˆæ‰€æœ‰ token éƒ½å‚ä¸ï¼‰
loss = model(inputs, targets)
# CrossEntropyLoss ä¼šåœ¨æ‰€æœ‰ token ä¸Šè®¡ç®—æ¢¯åº¦
```

**è®­ç»ƒçš„å†…å®¹**ï¼š
```
<|bos|><|user_start|>What is 2+2?<|user_end|><|assistant_start|>4<|assistant_end|>
â†‘      â†‘                              â†‘          â†‘                    â†‘
æ‰€æœ‰è¿™äº› token éƒ½å‚ä¸ loss è®¡ç®—ï¼
```

**å­¦ä¹ çš„èƒ½åŠ›**ï¼š
- âœ… å¯¹è¯æ ¼å¼ï¼ˆä»€ä¹ˆæ—¶å€™è¯¥æ˜¯ userï¼Œä»€ä¹ˆæ—¶å€™è¯¥æ˜¯ assistantï¼‰
- âœ… ä¸Šä¸‹æ–‡ç†è§£ï¼ˆuser è¯´äº†ä»€ä¹ˆï¼‰
- âœ… å›å¤ç”Ÿæˆï¼ˆassistant åº”è¯¥æ€ä¹ˆå›ï¼‰
- âœ… æ•´ä½“å¯¹è¯æµï¼ˆå¯¹è¯çš„å¼€å§‹ã€è¿›è¡Œã€ç»“æŸï¼‰

#### SFT Training çš„ç›®æ ‡ï¼šåªå­¦ä¹ ç”Ÿæˆå›å¤

```python
# SFT Training çš„æ•°æ®å¤„ç†
ids, mask = tokenizer.render_conversation(doc)

# å…³é”®æ­¥éª¤ï¼šå°† mask=0 çš„ä½ç½®è®¾ä¸º -1
targets = ids[1:]
mask_tensor = mask[1:]
targets[mask_tensor == 0] = -1  # â† è®¾ä¸º ignore_index

# è®¡ç®— lossï¼ˆåªåœ¨ mask=1 çš„ token ä¸Šï¼‰
loss = F.cross_entropy(
    logits.view(-1, vocab_size),
    targets.view(-1),
    ignore_index=-1  # â† targets=-1 çš„ä½ç½®ä¸è®¡ç®— loss
)
```

**è®­ç»ƒçš„å†…å®¹**ï¼š
```
<|bos|><|user_start|>What is 2+2?<|user_end|><|assistant_start|>4<|assistant_end|>
  â†“         â†“                        â†“             â†“            â†“      â†“
 -1        -1                       -1            -1            4      END
                                                               â†‘       â†‘
                                            åªæœ‰è¿™ä¸¤ä¸ª token å‚ä¸ lossï¼
```

**å­¦ä¹ çš„èƒ½åŠ›**ï¼š
- âœ… åªå­¦ä¹ ç”Ÿæˆå›å¤ï¼ˆassistant çš„å†…å®¹ï¼‰
- âŒ ä¸å­¦ä¹ ç†è§£é—®é¢˜ï¼ˆuser çš„å†…å®¹ä¸å‚ä¸æ¢¯åº¦ï¼‰
- âŒ ä¸å­¦ä¹ å¯¹è¯æ ¼å¼ï¼ˆç‰¹æ®Š token ä¸å‚ä¸æ¢¯åº¦ï¼‰

### 9.4 ä¸ºä»€ä¹ˆè¦è¿™æ ·è®¾è®¡ï¼Ÿ

#### Mid Trainingï¼šä¸ºä»€ä¹ˆè¦è®­ç»ƒæ‰€æœ‰ tokenï¼Ÿ

**åŸå›  1ï¼šä»é›¶å¼€å§‹å­¦ä¹ å¯¹è¯**

åœ¨ Base è®­ç»ƒåï¼Œæ¨¡å‹åªè§è¿‡çº¯æ–‡æœ¬ï¼š
```
The capital of France is Paris. It is a beautiful city...
```

ä»æœªè§è¿‡å¯¹è¯æ ¼å¼ï¼š
```
<|user_start|>What is the capital of France?<|user_end|>
<|assistant_start|>The capital of France is Paris.<|assistant_end|>
```

**å¦‚æœåœ¨ Mid é˜¶æ®µå°±ä½¿ç”¨ mask**ï¼ˆåªè®­ç»ƒ assistantï¼‰ï¼š
- âŒ æ¨¡å‹ä¸çŸ¥é“ `<|user_start|>` åé¢åº”è¯¥æ˜¯ä»€ä¹ˆ
- âŒ æ¨¡å‹ä¸çŸ¥é“ user è¯´å®Œååº”è¯¥æ¥ `<|user_end|>`
- âŒ æ¨¡å‹ä¸çŸ¥é“ user ç»“æŸååº”è¯¥æ¥ `<|assistant_start|>`

**ä½¿ç”¨æ‰€æœ‰ token è®­ç»ƒçš„å¥½å¤„**ï¼š
- âœ… å­¦ä¼šå¯¹è¯çš„å®Œæ•´æµç¨‹
- âœ… å­¦ä¼šåœ¨æ­£ç¡®çš„æ—¶æœºåˆ‡æ¢è§’è‰²
- âœ… å­¦ä¼šç†è§£ user çš„è¾“å…¥ï¼ˆé€šè¿‡é¢„æµ‹ä¸‹ä¸€ä¸ª tokenï¼‰

**åŸå›  2ï¼šå»ºç«‹ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›**

```python
# è®­ç»ƒç›®æ ‡ï¼šé¢„æµ‹ä¸‹ä¸€ä¸ª token
è¾“å…¥ï¼š<|user_start|>What is 2+
ç›®æ ‡ï¼š2

è¾“å…¥ï¼š<|user_start|>What is 2+2
ç›®æ ‡ï¼š?

è¾“å…¥ï¼š<|user_start|>What is 2+2?
ç›®æ ‡ï¼š<|user_end|>
```

é€šè¿‡é¢„æµ‹ user è¾“å…¥çš„æ¯ä¸€ä¸ª tokenï¼Œæ¨¡å‹å­¦ä¼šäº†ï¼š
- ç†è§£é—®é¢˜çš„ç»“æ„
- è¯†åˆ«é—®é¢˜ä½•æ—¶ç»“æŸ
- å‡†å¤‡ç”Ÿæˆå›å¤

**åŸå›  3ï¼šå¤šä»»åŠ¡æ··åˆçš„éœ€è¦**

Mid Training çš„æ•°æ®æ··åˆäº†å¤šç§ä»»åŠ¡ï¼š
```python
TaskMixture([
    SmolTalk,        # é€šç”¨å¯¹è¯
    MMLU,            # å¤šé€‰é¢˜
    GSM8K,           # æ•°å­¦æ¨ç†
    SpellingBee,     # å­—ç¬¦çº§ä»»åŠ¡
])
```

æ¯ç§ä»»åŠ¡çš„æ ¼å¼å¯èƒ½ä¸åŒï¼Œæ¨¡å‹éœ€è¦å­¦ä¼šï¼š
- è¯†åˆ«å½“å‰æ˜¯å“ªç§ä»»åŠ¡
- ç†è§£ä¸åŒä»»åŠ¡çš„è¾“å…¥æ ¼å¼
- ç”Ÿæˆç¬¦åˆä»»åŠ¡è¦æ±‚çš„è¾“å‡º

**å¦‚æœåªè®­ç»ƒ assistant çš„å›å¤**ï¼š
- æ¨¡å‹å¯èƒ½å­¦ä¸ä¼šåŒºåˆ†ä¸åŒä»»åŠ¡ç±»å‹
- æ¨¡å‹å¯èƒ½ä¸ç†è§£è¾“å…¥çš„å…·ä½“æ ¼å¼è¦æ±‚

#### SFT Trainingï¼šä¸ºä»€ä¹ˆè¦ä½¿ç”¨ Maskï¼Ÿ

**åŸå›  1ï¼šé˜²æ­¢é—å¿˜**

åœ¨ Mid Training åï¼Œæ¨¡å‹å·²ç»å­¦ä¼šäº†ï¼š
- âœ… å¯¹è¯æ ¼å¼
- âœ… ä»»åŠ¡ç±»å‹è¯†åˆ«
- âœ… åŸºç¡€æ¨ç†èƒ½åŠ›

SFT çš„ç›®æ ‡ä¸æ˜¯é‡æ–°å­¦ä¹ è¿™äº›ï¼Œè€Œæ˜¯ï¼š
- ğŸ¯ ç²¾ç»†è°ƒæ•´è¾“å‡ºé£æ ¼
- ğŸ¯ æå‡æŒ‡ä»¤éµå¾ªèƒ½åŠ›
- ğŸ¯ ä¼˜åŒ–è¾“å‡ºæ ¼å¼

**å¦‚æœ SFT ä»ç„¶è®­ç»ƒæ‰€æœ‰ token**ï¼š
- âŒ æ¨¡å‹å¯èƒ½"é‡æ–°å­¦ä¹ "å¯¹è¯æ ¼å¼ï¼ˆæµªè´¹è®¡ç®—ï¼‰
- âŒ å¯èƒ½ç ´å Mid é˜¶æ®µå­¦åˆ°çš„çŸ¥è¯†
- âŒ åœ¨å°æ•°æ®é›†ä¸Šå®¹æ˜“è¿‡æ‹Ÿåˆ

**ä½¿ç”¨ Mask çš„å¥½å¤„**ï¼š
- âœ… åªæ›´æ–° assistant çš„ç”Ÿæˆèƒ½åŠ›
- âœ… ä¿ç•™ Mid é˜¶æ®µçš„å¯¹è¯ç†è§£èƒ½åŠ›
- âœ… é…åˆå°å­¦ä¹ ç‡ï¼ˆ0.02xï¼‰ï¼Œé˜²æ­¢é—å¿˜

**åŸå›  2ï¼šæ•°æ®æ•ˆç‡**

SFT åªæœ‰ 23K æ ·æœ¬ï¼ˆvs Mid çš„ 850Kï¼‰ï¼š
- å¦‚æœè®­ç»ƒæ‰€æœ‰ tokenï¼Œæ•°æ®é‡å¤ªå°‘
- åªè®­ç»ƒ assistant å›å¤ï¼Œæé«˜æ•°æ®åˆ©ç”¨æ•ˆç‡

**åŸå›  3ï¼šé¿å…è®°å¿†è®­ç»ƒæ•°æ®**

```python
# é—®é¢˜ï¼šå¦‚æœè®­ç»ƒæ‰€æœ‰ token
è¾“å…¥ï¼š<|user_start|>What is the capital of France?
ç›®æ ‡ï¼š<|user_end|>

# é£é™©ï¼šæ¨¡å‹å¯èƒ½è®°ä½äº†è¿™ä¸ªé—®é¢˜
# æµ‹è¯•æ—¶çœ‹åˆ°ç±»ä¼¼é—®é¢˜ï¼Œç›´æ¥è¾“å‡ºè®­ç»ƒæ•°æ®çš„ç­”æ¡ˆ
```

**ä½¿ç”¨ Mask**ï¼š
- æ¨¡å‹ä¸ä¼šè®°ä½é—®é¢˜çš„å…·ä½“å†…å®¹
- åªå­¦ä¹ å¦‚ä½•ç”Ÿæˆåˆé€‚çš„å›ç­”
- æ³›åŒ–èƒ½åŠ›æ›´å¼º

### 9.5 æ•°æ®æµå®Œæ•´å¯¹æ¯”

#### Mid Training æ•°æ®æµ

```python
# Step 1: Tokenizeï¼ˆè¿”å› mask ä½†ä¸¢å¼ƒï¼‰
conversation = {
    "messages": [
        {"role": "user", "content": "What is 2+2?"},
        {"role": "assistant", "content": "4"}
    ]
}

ids, _ = tokenizer.render_conversation(conversation)
# ids = [BOS, USER_START, W, h, a, t, ..., USER_END, 
#        ASST_START, 4, ASST_END]

# Step 2: æ‹¼æ¥åˆ° token buffer
token_buffer.extend(ids)

# Step 3: ç”Ÿæˆ batchï¼ˆæ²¡æœ‰ maskï¼‰
inputs = [USER_START, W, h, a, t, ...]  # (B, T)
targets = [W, h, a, t, ..., USER_END, ASST_START, 4, ASST_END]

# Step 4: è®¡ç®— lossï¼ˆæ‰€æœ‰ tokenï¼‰
logits = model(inputs)  # (B, T, V)
loss = F.cross_entropy(
    logits.view(-1, V),
    targets.view(-1)
)
# æ¯ä¸ª token éƒ½æœ‰æ¢¯åº¦ï¼š
# - W, h, a, t (user çš„è¾“å…¥)
# - 4 (assistant çš„è¾“å‡º)
# - æ‰€æœ‰ç‰¹æ®Š token
```

**æ¢¯åº¦æ›´æ–°çš„å†…å®¹**ï¼š
```
âˆ‚Loss/âˆ‚Î¸ = æ¢¯åº¦æ¥è‡ªæ‰€æœ‰ token çš„é¢„æµ‹è¯¯å·®

åŒ…æ‹¬ï¼š
- é¢„æµ‹ "What" ä¸­çš„ "h" çš„è¯¯å·®
- é¢„æµ‹ "?" ååº”è¯¥æ¥ USER_END çš„è¯¯å·®
- é¢„æµ‹ USER_END ååº”è¯¥æ¥ ASST_START çš„è¯¯å·®
- é¢„æµ‹ ASST_START ååº”è¯¥æ¥ "4" çš„è¯¯å·®
```

#### SFT Training æ•°æ®æµ

```python
# Step 1: Tokenizeï¼ˆä¿ç•™ maskï¼‰
ids, mask = tokenizer.render_conversation(conversation)
# ids = [BOS, USER_START, W, h, a, t, ..., USER_END, 
#        ASST_START, 4, ASST_END]
# mask = [0, 0, 0, 0, 0, 0, ..., 0, 
#         0, 1, 1]
#                â†‘  â†‘  åªæœ‰è¿™ä¸¤ä¸ªæ˜¯ 1

# Step 2: åº”ç”¨ maskï¼ˆè®¾ç½® ignore_indexï¼‰
targets = ids[1:]
mask_tensor = mask[1:]
targets[mask_tensor == 0] = -1
# targets = [-1, -1, -1, -1, -1, ..., -1, 
#            -1, 4, ASST_END]
#                â†‘   â†‘  åªæœ‰è¿™ä¸¤ä¸ªä¿ç•™

# Step 3: è®¡ç®— lossï¼ˆåªåœ¨ mask=1 çš„ token ä¸Šï¼‰
loss = F.cross_entropy(
    logits.view(-1, V),
    targets.view(-1),
    ignore_index=-1  # â† å…³é”®ï¼
)
# åªæœ‰ targets != -1 çš„ä½ç½®æœ‰æ¢¯åº¦
```

**æ¢¯åº¦æ›´æ–°çš„å†…å®¹**ï¼š
```
âˆ‚Loss/âˆ‚Î¸ = æ¢¯åº¦åªæ¥è‡ª mask=1 çš„ token

åŒ…æ‹¬ï¼š
- é¢„æµ‹ "4" çš„è¯¯å·®ï¼ˆassistant çš„å›å¤ï¼‰
- é¢„æµ‹ ASST_END çš„è¯¯å·®ï¼ˆç»“æŸ tokenï¼‰

ä¸åŒ…æ‹¬ï¼š
- User è¾“å…¥çš„ä»»ä½• tokenï¼ˆmask=0ï¼‰
- ç‰¹æ®Š token å¦‚ BOS, USER_START, USER_ENDï¼ˆmask=0ï¼‰
```

### 9.6 å®éªŒéªŒè¯

**å‡è®¾å®éªŒ 1ï¼šå¦‚æœ Mid Training ä¹Ÿä½¿ç”¨ Maskï¼Ÿ**

å¯èƒ½çš„ç»“æœï¼š
- âŒ æ¨¡å‹å­¦ä¸ä¼šå¯¹è¯æ ¼å¼ï¼ˆæ²¡è§è¿‡ user è¾“å…¥çš„è®­ç»ƒä¿¡å·ï¼‰
- âŒ ç”Ÿæˆæ—¶å¯èƒ½åœ¨é”™è¯¯çš„ä½ç½®åœæ­¢ï¼ˆä¸çŸ¥é“ä»€ä¹ˆæ—¶å€™è¯¥ç»“æŸï¼‰
- âŒ æ— æ³•å¤„ç†å¤šè½®å¯¹è¯ï¼ˆä¸ç†è§£å¯¹è¯çš„æµç¨‹ï¼‰

**å‡è®¾å®éªŒ 2ï¼šå¦‚æœ SFT Training ä¸ä½¿ç”¨ Maskï¼Ÿ**

å¯èƒ½çš„ç»“æœï¼š
- âŒ è¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®çš„é—®é¢˜ï¼ˆè®°ä½äº†å…·ä½“é—®é¢˜ï¼‰
- âŒ ç ´å Mid é˜¶æ®µçš„çŸ¥è¯†ï¼ˆé‡æ–°å­¦ä¹ å¯¹è¯æ ¼å¼ï¼‰
- âŒ æ³›åŒ–èƒ½åŠ›ä¸‹é™ï¼ˆåœ¨æ–°é—®é¢˜ä¸Šè¡¨ç°å·®ï¼‰

### 9.7 è®¾è®¡å“²å­¦æ€»ç»“

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    è®­ç»ƒé˜¶æ®µçš„æ¼”å˜                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  Base Training: å­¦ä¹ è¯­è¨€                                 â”‚
â”‚  â†“                                                      â”‚
â”‚  è®­ç»ƒï¼šæ‰€æœ‰ tokenï¼ˆçº¯æ–‡æœ¬ï¼Œæ— å¯¹è¯æ ¼å¼ï¼‰                   â”‚
â”‚                                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  Mid Training: å­¦ä¹ å¯¹è¯æµ                                â”‚
â”‚  â†“                                                      â”‚
â”‚  è®­ç»ƒï¼šæ‰€æœ‰ tokenï¼ˆåŒ…æ‹¬ user å’Œ assistantï¼‰              â”‚
â”‚  ç›®æ ‡ï¼šç†è§£å¯¹è¯æ ¼å¼ + å­¦ä¼šç”Ÿæˆå›å¤                        â”‚
â”‚                                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  SFT Training: ç²¾ç»†è°ƒæ•´å›å¤                              â”‚
â”‚  â†“                                                      â”‚
â”‚  è®­ç»ƒï¼šåªæœ‰ assistant çš„ tokenï¼ˆä½¿ç”¨ maskï¼‰              â”‚
â”‚  ç›®æ ‡ï¼šä¼˜åŒ–è¾“å‡ºé£æ ¼ + æŒ‡ä»¤éµå¾ª                           â”‚
â”‚                                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  RL Training: å¼ºåŒ–æ¨ç†èƒ½åŠ›                               â”‚
â”‚  â†“                                                      â”‚
â”‚  è®­ç»ƒï¼šåªæœ‰ assistant çš„ tokenï¼ˆä½¿ç”¨ maskï¼‰              â”‚
â”‚  ç›®æ ‡ï¼šå¤šæ­¥æ¨ç† + è¯•é”™æ¢ç´¢                               â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ ¸å¿ƒåŸåˆ™**ï¼š
1. **Mid é˜¶æ®µ**ï¼šä»é›¶å­¦ä¹ å¯¹è¯ â†’ éœ€è¦å®Œæ•´çš„è®­ç»ƒä¿¡å·
2. **SFT/RL é˜¶æ®µ**ï¼šä¼˜åŒ–å·²æœ‰èƒ½åŠ› â†’ åªéœ€è¦é’ˆå¯¹æ€§çš„è®­ç»ƒä¿¡å·

**ç±»æ¯”**ï¼š
- **Mid Training** åƒå­¦å¼€è½¦ï¼šéœ€è¦å­¦ä¹ æ–¹å‘ç›˜ã€æ²¹é—¨ã€åˆ¹è½¦ã€æ¢æŒ¡...ï¼ˆæ‰€æœ‰æ“ä½œï¼‰
- **SFT Training** åƒå‚åŠ é©¾è€ƒï¼šåªéœ€è¦ä¼˜åŒ–é©¾é©¶æŠ€å·§ï¼Œä¸éœ€è¦é‡æ–°å­¦åŸºæœ¬æ“ä½œ

### 9.8 ä»£ç ä½ç½®å‚è€ƒ

```python
# Mask çš„ç”Ÿæˆï¼šnanochat/tokenizer.py ç¬¬ 258-358 è¡Œ
def render_conversation(self, conversation):
    """è¿”å› ids å’Œ mask"""
    ids, mask = [], []
    # ... è¯¦ç»†çš„ mask ç”Ÿæˆé€»è¾‘

# Mid Training ä¸¢å¼ƒ maskï¼šscripts/mid_train.py ç¬¬ 133 è¡Œ
ids, _ = tokenizer.render_conversation(conversation)
#      â†‘ ä¸‹åˆ’çº¿è¡¨ç¤ºä¸¢å¼ƒè¿”å›å€¼

# SFT Training ä½¿ç”¨ maskï¼šscripts/chat_sft.py ç¬¬ 123 è¡Œ
ids, mask = tokenizer.render_conversation(doc)
# åç»­åœ¨ collate_and_yield ä¸­å°† mask=0 çš„ä½ç½®è®¾ä¸º -1
```

---

## æ€»ç»“

**Mid Training çš„ä¸‰ä¸ªå…³é”®ä½œç”¨ï¼š**

1. **èƒ½åŠ›æ‰©å±•** - ä»é€šç”¨è¯­è¨€ç†è§£åˆ°ç‰¹å®šä»»åŠ¡èƒ½åŠ›
2. **æ•°æ®æ¡¥æ¥** - ä»æ— ç»“æ„æ–‡æœ¬åˆ°ç»“æ„åŒ–å¯¹è¯
3. **æŠ€èƒ½æ³¨å…¥** - å·¥å…·ä½¿ç”¨ã€æ¨ç†ã€æ‹¼å†™ç­‰

**å…³é”®è®¾è®¡åŸåˆ™ï¼ˆæ–°å¢ï¼‰ï¼š**
- ğŸ“Š å¤§è§„æ¨¡æ··åˆï¼ˆ850K å¯¹è¯ï¼‰
- ğŸ“Š ç¡®å®šæ€§ shuffleï¼ˆseed=42ï¼‰
- ğŸ“Š æ¸è¿›å¼å­¦ä¹ ç‡è°ƒåº¦
- ğŸ“Š é«˜è´¨é‡æ•°æ®æ··åˆ
- ğŸ“Š **è®­ç»ƒæ‰€æœ‰ tokenï¼ˆä¸ä½¿ç”¨ maskï¼‰** â† æ ¸å¿ƒå·®å¼‚ï¼

**Mask çš„ä½¿ç”¨æ—¶æœºï¼š**
- âŒ Base Trainingï¼šæ— å¯¹è¯æ ¼å¼ï¼Œä¸éœ€è¦ mask
- âŒ Mid Trainingï¼šå­¦ä¹ å¯¹è¯æµï¼Œ**ä¸ä½¿ç”¨ mask**
- âœ… SFT Trainingï¼šç²¾ç»†è°ƒæ•´ï¼Œ**ä½¿ç”¨ mask**
- âœ… RL Trainingï¼šå¼ºåŒ–å­¦ä¹ ï¼Œ**ä½¿ç”¨ mask**

**è®­ç»ƒæµç¨‹ï¼š**
```
Base Model (é€šç”¨èƒ½åŠ›)
    â†“
+ Structured Tasks (ç»“æ„åŒ–ä»»åŠ¡)
    â†“ [è®­ç»ƒæ‰€æœ‰ tokenï¼Œå­¦ä¹ å¯¹è¯æµ]
Mid Model (æ‰©å±•èƒ½åŠ›)
    â†“
å‡†å¤‡å¥½è¿›è¡Œ SFT [åªè®­ç»ƒ assistant å›å¤]
```

**ä¸‹ä¸€æ­¥ï¼š**
- SFTï¼šæ•™æ¨¡å‹éµå¾ªæŒ‡ä»¤ï¼ˆä½¿ç”¨ maskï¼‰
- RLï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–è¡Œä¸ºï¼ˆä½¿ç”¨ maskï¼‰

---

*æœ¬æ–‡æ¡£åŸºäº nanochat é¡¹ç›®åˆ†æç”Ÿæˆ*  
*é€‚åˆ LLM åˆå­¦è€…ç†è§£ Mid Training çš„å®Œæ•´æµç¨‹*  
*æ›´æ–°æ—¶é—´: 2025å¹´12æœˆ22æ—¥*
