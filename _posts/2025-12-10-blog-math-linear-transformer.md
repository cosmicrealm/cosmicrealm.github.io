---
title: '3Blue1Brown 线性代数笔记'
date: 2025-12-10
permalink: /posts/2025/12/2025-12-10-blog-math-linear-transformer/
tags:
  - math
---

### 3Blue1Brown 线性代数笔记（几何直觉）

本文基于 3Blue1Brown 的线性代数视频系列整理，目标不是推导公式，而是通过“空间与几何直觉”来理解线性代数的核心概念。  

---

## 0. 线性代数研究的核心问题

线性代数研究的问题可以概括为：

在一个空间中，向量如何叠加？  
空间如何在“线性变换”作用下发生拉伸、旋转、压缩与翻转？

核心研究对象只有三类：

- 向量（Vector）
- 线性变换（Linear Transformation）
- 矩阵（Matrix）：线性变换在某一组坐标基下的表示

本质上，线性代数并不是“在算矩阵”，而是在研究：

空间的结构，以及空间在变换下如何被重新塑形。

---

## 1. 向量的本质

### 1.1 物理视角

向量是空间中的一个“箭头”，由两个属性唯一确定：

- 长度（大小）
- 方向

在几何中，只要长度和方向相同，即使位置不同，本质上仍然是同一个向量。

---

### 1.2 计算机视角

在计算机中，向量通常表示为一个有序数列：

$$
(x_1, x_2, \dots, x_n)
$$

顺序不可改变，因为每一维通常对应不同的物理或逻辑意义。

---

### 1.3 数学视角（抽象向量）

从更抽象的角度看：

只要某一类对象满足以下两种运算：

- 向量加法
- 数乘（标量乘法）

并且满足：

- 可加性：  
$$
u + v = v + u
$$

- 线性叠加性：  
$$
T(u+v) = T(u) + T(v)
$$

- 齐次性：  
$$
T(cu) = cT(u)
$$

那么，这类对象就可以被看作“向量”。

因此，函数、多项式、信号、随机变量等，在数学上都可以作为向量来研究。

---

## 2. 向量加法与数乘的几何意义

### 2.1 向量加法

设有两个向量：

$$
u = (x_1, y_1), \quad v = (x_2, y_2)
$$

它们的加法定义为：

$$
u + v = (x_1 + x_2, y_1 + y_2)
$$

几何意义是：  
先沿 $$u$$ 的方向移动，再沿 $$v$$ 的方向移动，最终从原点直达终点。

---

### 2.2 数乘

设 $$c$$ 是一个标量，则：

$$
cu = (cx, cy)
$$

几何意义是：

- $$c > 1$$：拉伸
- $$c < 1$$：压缩
- $$ c < 0 $$：方向翻转

---

## 3. 线性组合、张成空间与基

### 3.1 线性组合

给定向量 $$v_1, v_2, \dots, v_k$$，任意形如：

$$
a_1v_1 + a_2v_2 + \dots + a_kv_k
$$

的表达式，称为这些向量的一个线性组合。

---

### 3.2 张成空间（Span）

所有线性组合构成的集合，称为这些向量张成的空间。

几何含义：

- 两个不共线的向量在二维中张成整个平面
- 共线向量只能张成一条直线
- 一个非零向量只能张成过原点的一条直线

---

### 3.3 线性相关与线性无关

如果在一组向量中，存在某一个向量可以由其它向量线性组合得到，则这组向量线性相关。  
如果每一个向量都不能由其它向量线性组合得到，则线性无关。

---

### 3.4 基（Basis）

基是满足以下两个条件的向量集合：

- 线性无关
- 能张成整个空间

二维标准基为：

$$
\mathbf{i} = (1,0), \quad \mathbf{j} = (0,1)
$$

任意二维向量：

$$
(x,y) = x\mathbf{i} + y\mathbf{j}
$$

---

## 4. 线性变换与矩阵

### 4.1 线性变换的定义

一个变换 $$T$$ 是线性的，当且仅当满足：

$$
T(u+v) = T(u) + T(v)
$$

$$
T(cu) = cT(u)
$$

并且原点保持不变。

---

### 4.2 矩阵的几何本质

设二维矩阵：

$$
A =
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}
$$

它表示：

$$
A\mathbf{i} = (a,c), \quad A\mathbf{j} = (b,d)
$$

对任意向量：

$$
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}
\begin{bmatrix}
x \\ y
\end{bmatrix}
= xA\mathbf{i} + yA\mathbf{j}
$$

矩阵的列向量就是变换后的基向量。

---

## 5. 矩阵乘法与变换复合

$$
(AB)v = A(Bv)
$$

含义是：右边的变换先作用，左边的后作用。

因此：

- 矩阵乘法满足结合律
- 一般不满足交换律

---

## 6. 行列式的几何意义

在二维中：

$$
\det(A)
$$

表示单位正方形在变换 $A$ 作用下，面积的缩放比例。

- $$\det(A) > 0$$：保持方向  
- $$\det(A) < 0$$：发生翻转  
- $$\det(A) = 0$$：空间被压缩到更低维

在三维中，行列式表示体积缩放比例。

乘积法则：

$$
\det(M_1M_2) = \det(M_1)\det(M_2)
$$

---

## 7. 线性方程组、逆矩阵与零空间

求解：

$$
Ax = v
$$

等价于：寻找一个向量 $$x$$，使得线性变换 $$A$$ 把它变成 $$v$$。

- 若 $$\det(A) \neq 0$$，则存在唯一解，且有逆矩阵：

$$
A^{-1}A = I
$$

- 若 $$\det(A) = 0$$，则空间发生降维：
  - 部分 $$v$$ 无解
  - 部分 $$v$$ 有无穷多解

零空间定义为：

$$
\{x \mid Ax = 0\}
$$

---

## 8. 列空间与秩

- 列空间是所有 $$Av$$ 的集合
- 秩（Rank）是列空间的维度

性质：

- 满秩表示不降维
- 非满秩表示发生降维

---

## 9. 点积与对偶性

点积定义：

$$
u \cdot v = \|u\|\|v\|\cos\theta
$$

几何意义：投影关系。

任意从空间到数轴的线性变换，都等价于与某个向量的点积：

$$
f(v) = w \cdot v
$$

---

## 10. 叉积

三维空间中：

$$
u \times v
$$

是一个垂直于 $$u,v$$ 的向量，其长度等于平行四边形面积。

反对称性：

$$
u \times v = - v \times u
$$

二维中叉积的“大小”等于行列式。

---

## 11. 基变换与相似变换

$$
A^{-1}MA
$$

表示同一个变换在不同坐标系下的描述。

---

## 12. 特征值与特征向量

定义：

$$
Av = \lambda v
$$

其中 $$v$$  是特征向量，$$\lambda$$ 是特征值。

特征值由方程确定：

$$
\det(A - \lambda I) = 0
$$

若特征向量能张满空间，则矩阵可对角化，便于计算幂次与动力系统。

---

# 附录：线性代数核心名词与概念定义表（Glossary）

- 向量（Vector）：带有大小和方向的数学对象，或满足加法与数乘的抽象对象。
- 线性变换（Linear Transformation）：保持加法与数乘结构的空间映射。
- 矩阵（Matrix）：线性变换在某组基下的坐标表示。
- 线性组合（Linear Combination）：向量按标量加权后相加。
- 张成空间（Span）：所有线性组合构成的集合。
- 线性无关（Linear Independence）：任一向量都不能由其它向量线性组合得到。
- 基（Basis）：线性无关且张成整个空间的向量集合。
- 行列式（Determinant）：线性变换对面积或体积的缩放比例。
- 列空间（Column Space）：矩阵所有可能输出构成的空间。
- 秩（Rank）：列空间的维度。
- 零空间（Null Space）：被变换压缩到原点的所有向量集合。
- 点积（Dot Product）：两个向量的投影度量。
- 叉积（Cross Product）：三维中产生垂直向量的运算。
- 特征向量（Eigenvector）：变换后方向不变的向量。
- 特征值（Eigenvalue）：对应特征向量的缩放比例。
- 相似变换（Similarity Transform）：不同坐标系下对同一变换的表示方式。

---