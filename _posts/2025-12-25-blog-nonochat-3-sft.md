---
title: 'noao-chat-sfté˜¶æ®µè®­ç»ƒ'
date: 2025-12-25
permalink: /posts/2025/12/2025-12-25-blog-nonochat-sft/
tags:
  - llm
---

### LLM SFT è®­ç»ƒå®Œæ•´è§£æ

[nonochat](https://github.com/karpathy/nanochat)



> æœ¬æ–‡æ¡£è¯¦ç»†è§£æ `chat_sft.py`ï¼Œè®²è§£ç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuningï¼‰å¦‚ä½•å°†æ¨¡å‹å˜æˆå¯¹è¯åŠ©æ‰‹ã€‚  
> é€‚åˆ LLM é¢†åŸŸçš„åˆå­¦è€…ï¼Œä»èƒ½åŠ›æ¨¡å‹åˆ°åº”ç”¨æ¨¡å‹çš„å…³é”®ä¸€æ­¥ã€‚

---

## ç›®å½•

1. [ä»€ä¹ˆæ˜¯ SFT](#1-ä»€ä¹ˆæ˜¯-sft)
2. [Mask æœºåˆ¶çš„å¥¥ç§˜](#2-mask-æœºåˆ¶çš„å¥¥ç§˜)
3. [è®­ç»ƒæµç¨‹è¯¦è§£](#3-è®­ç»ƒæµç¨‹è¯¦è§£)
4. [è¯„ä¼°ä½“ç³»](#4-è¯„ä¼°ä½“ç³»)
5. [ä¸å‰åºé˜¶æ®µçš„å¯¹æ¯”](#5-ä¸å‰åºé˜¶æ®µçš„å¯¹æ¯”)

---

## 1. ä»€ä¹ˆæ˜¯ SFT

### 1.1 è®­ç»ƒé˜¶æ®µå…¨æ™¯

```
Base Training    â†’    Mid Training    â†’    SFT           â†’    RL
     â†“                     â†“                  â†“                  â†“
é€šç”¨è¯­è¨€èƒ½åŠ›         ç‰¹å®šä»»åŠ¡èƒ½åŠ›         æŒ‡ä»¤éµå¾ª        åå¥½å¯¹é½
(æµ·é‡æ–‡æœ¬)         (ç»“æ„åŒ–ä»»åŠ¡)        (å¯¹è¯å¾®è°ƒ)      (å¼ºåŒ–å­¦ä¹ )
  æ•°å‘¨è®­ç»ƒ              æ•°å°æ—¶              æ•°å°æ—¶          æ•°å°æ—¶
  æ•°åäº¿tokens        æ•°ç™¾ä¸‡å¯¹è¯          æ•°ä¸‡å¯¹è¯        æ•°åƒå¯¹è¯
```

**SFT çš„å®šä½ï¼š** è®­ç»ƒæµç¨‹çš„ç¬¬ä¸‰é˜¶æ®µï¼Œå°†æ¨¡å‹ä»"èƒ½åŠ›å‹"è½¬å˜ä¸º"åº”ç”¨å‹"ã€‚

### 1.2 SFT è¦è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ

**Mid æ¨¡å‹çš„èƒ½åŠ›ä¸é™åˆ¶ï¼š**

æµ‹è¯• Mid æ¨¡å‹ï¼š
```
è¾“å…¥: "What is 2+2?"
Mid æ¨¡å‹å¯èƒ½è¾“å‡º: "2+2=4. Four is a number..."
```

é—®é¢˜ï¼š
- âœ… çŸ¥é“ç­”æ¡ˆæ˜¯ 4
- âŒ å›ç­”å¤ªéšæ„ï¼Œä¸å¤Ÿç®€æ´
- âŒ ä¸éµå¾ªç‰¹å®šæ ¼å¼
- âŒ å¯èƒ½ç»§ç»­ç”Ÿæˆæ— å…³å†…å®¹

**æˆ‘ä»¬æœŸæœ›çš„è¾“å‡ºï¼š**
```
è¾“å…¥: "What is 2+2?"
SFT æ¨¡å‹è¾“å‡º: "The answer is 4."
```

ç‰¹ç‚¹ï¼š
- âœ… ç®€æ´æ˜äº†
- âœ… ç›´æ¥å›ç­”é—®é¢˜
- âœ… éµå¾ªè‰¯å¥½çš„å¯¹è¯ç¤¼ä»ª
- âœ… çŸ¥é“ä½•æ—¶åœæ­¢

### 1.3 SFT çš„æ ¸å¿ƒç›®æ ‡

**ä¸‰ä¸ªå…³é”®èƒ½åŠ›ï¼š**

1. **æŒ‡ä»¤éµå¾ª (Instruction Following)**
   ```
   ç”¨æˆ·: "ç”¨ä¸‰ä¸ªè¯æ€»ç»“è¿™ç¯‡æ–‡ç« "
   æ¨¡å‹: "åˆ›æ–°ã€æŠ€æœ¯ã€æœªæ¥"  â† ä¸¥æ ¼éµå¾ª"ä¸‰ä¸ªè¯"çš„è¦æ±‚
   ```

2. **æ ¼å¼æ§åˆ¶ (Format Control)**
   ```
   ç”¨æˆ·: "ä»¥ JSON æ ¼å¼å›ç­”"
   æ¨¡å‹: {"answer": "42", "reason": "..."}  â† ç²¾ç¡®çš„æ ¼å¼
   ```

3. **å¯¹è¯ç¤¼ä»ª (Conversational Etiquette)**
   ```
   ç”¨æˆ·: "è°¢è°¢ï¼"
   æ¨¡å‹: "ä¸å®¢æ°”ï¼æœ‰å…¶ä»–é—®é¢˜éšæ—¶é—®æˆ‘ã€‚"  â† å‹å¥½ã€å¾—ä½“
   ```

### 1.4 SFT çš„æ•°æ®ç‰¹ç‚¹

**æ•°æ®é‡å¯¹æ¯”ï¼š**

| é˜¶æ®µ | æ•°æ®é‡ | æ•°æ®ç±»å‹ |
|-----|--------|---------|
| Base | æ•°åäº¿ tokens | ç½‘é¡µæ–‡æœ¬ |
| Mid | æ•°ç™¾ä¸‡å¯¹è¯ | ä»»åŠ¡æ•°æ®é›† |
| **SFT** | **æ•°ä¸‡å¯¹è¯** | **ç²¾å¿ƒæ ‡æ³¨** |
| RL | æ•°åƒå¯¹è¯ | ç¯å¢ƒåé¦ˆ |

**SFT æ•°æ®çš„é»„é‡‘æ ‡å‡†ï¼š**
- ğŸ“ æ¯æ¡å¯¹è¯éƒ½æ˜¯**äººå·¥ç¼–å†™æˆ–å®¡æ ¸**
- âœ¨ å±•ç¤º**æœ€ä½³å®è·µ**ï¼ˆä¸æ˜¯éšä¾¿çš„å¯¹è¯ï¼‰
- ğŸ¯ æ¶µç›–**åº”ç”¨åœºæ™¯**ï¼ˆå®é™…ç”¨æˆ·ä¼šé—®ä»€ä¹ˆï¼‰
- ğŸ† ä½“ç°**æœŸæœ›è¡Œä¸º**ï¼ˆæ¨¡å‹åº”è¯¥æ€ä¹ˆå›ç­”ï¼‰

### 1.5 æœ¬é¡¹ç›®çš„ SFT é…ç½®

```python
# è®­ç»ƒæ•°æ®æ··åˆ (æ€»å…± ~23K å¯¹è¯)
train_ds = TaskMixture([
    ARC(subset="ARC-Easy", split="train"),      # 2.3K ç®€å•æ¨ç†
    ARC(subset="ARC-Challenge", split="train"), # 1.1K å›°éš¾æ¨ç†
    GSM8K(subset="main", split="train"),        # 8K æ•°å­¦é¢˜
    SmolTalk(split="train", stop=10_000),       # 10K å¯¹è¯
    CustomJSON(identity_file),                  # 1K èº«ä»½å¯¹è¯
    SimpleSpelling(size=300, split="train"),    # 300 æ‹¼å†™
    SpellingBee(size=300, split="train"),       # 300 å­—æ¯è®¡æ•°
])
```

**æ•°æ®ç‰¹ç‚¹ï¼š**
- ç›¸æ¯” Mid Training (850K)ï¼Œæ•°æ®é‡å‡å°‘äº† **97%**
- ä½†éƒ½æ˜¯**é«˜è´¨é‡**çš„å¯¹è¯ç¤ºä¾‹
- æ¶µç›–å¤šç§ä»»åŠ¡ç±»å‹
- æ¯ä¸ªä»»åŠ¡éƒ½æœ‰æ˜ç¡®çš„æœŸæœ›è¾“å‡ºæ ¼å¼

---

## 2. Mask æœºåˆ¶çš„å¥¥ç§˜

### 2.1 ä¸ºä»€ä¹ˆéœ€è¦ Maskï¼Ÿ

**é—®é¢˜åœºæ™¯ï¼š**

å‡è®¾ä¸€ä¸ªå¯¹è¯ï¼š
```
User: "What is the capital of France?"
Assistant: "The capital of France is Paris."
```

**å¦‚æœæ²¡æœ‰ Maskï¼ˆå…¨éƒ¨è®­ç»ƒï¼‰ï¼š**

```python
# æ‰€æœ‰ token éƒ½è®¡ç®—æŸå¤±
tokens = [<|bos|>, <|user_start|>, "What", "is", "the", "capital", 
          "of", "France", "?", <|user_end|>, <|assistant_start|>, 
          "The", "capital", "of", "France", "is", "Paris", ".", 
          <|assistant_end|>]

# æ¨¡å‹éœ€è¦å­¦ä¹ ï¼š
# - é¢„æµ‹ <|user_start|> ä¹‹åæ˜¯ "What"  â† ä¸ºä»€ä¹ˆè¦å­¦è¿™ä¸ªï¼Ÿ
# - é¢„æµ‹ "What" ä¹‹åæ˜¯ "is"            â† ç”¨æˆ·æ€ä¹ˆè¯´è¯ä¸é‡è¦ï¼
# - é¢„æµ‹ "?" ä¹‹åæ˜¯ <|user_end|>        â† è¿™æ˜¯å›ºå®šæ ¼å¼
```

**é—®é¢˜ï¼š**
- âŒ æµªè´¹è®­ç»ƒèµ„æºå­¦ä¹ ç”¨æˆ·çš„è¯´è¯æ–¹å¼
- âŒ æ¨¡å‹å¯èƒ½å­¦ä¼šç”Ÿæˆç”¨æˆ·æ¶ˆæ¯ï¼ˆè§’è‰²æ··æ·†ï¼‰
- âŒ è®­ç»ƒä¿¡å·è¢«ç¨€é‡Š

**Mask çš„è§£å†³æ–¹æ¡ˆï¼š**

```python
tokens = [<|bos|>, <|user_start|>, "What", "is", ..., <|user_end|>, 
          <|assistant_start|>, "The", "capital", ..., <|assistant_end|>]

mask   = [0,       0,              0,      0,   ..., 0,
          0,                       1,      1,        ..., 1]
          â†‘                        â†‘
          ä¸è®­ç»ƒ                   è®­ç»ƒï¼
```

**æ•ˆæœï¼š**
- âœ… åªå­¦ä¹ å¦‚ä½•ç”Ÿæˆ Assistant çš„å›å¤
- âœ… ä¸“æ³¨äºé‡è¦çš„è®­ç»ƒä¿¡å·
- âœ… é¿å…è§’è‰²æ··æ·†

### 2.2 Mask çš„è¯¦ç»†è§„åˆ™

#### è§„åˆ™æ€»è§ˆ

| Token ç±»å‹ | Mask å€¼ | æ˜¯å¦è®­ç»ƒ | åŸå›  |
|-----------|---------|----------|------|
| `<|bos|>` | 0 | âŒ | æ–‡æ¡£å¼€å§‹æ ‡è®° |
| `<|user_start|>` | 0 | âŒ | ç”¨æˆ·æ¶ˆæ¯å¼€å§‹ |
| ç”¨æˆ·æ¶ˆæ¯å†…å®¹ | 0 | âŒ | ç”¨æˆ·è¾“å…¥ï¼Œä¸éœ€è¦å­¦ä¹  |
| `<|user_end|>` | 0 | âŒ | ç”¨æˆ·æ¶ˆæ¯ç»“æŸ |
| `<|assistant_start|>` | 0 | âŒ | åŠ©æ‰‹å¼€å§‹æ ‡è®° |
| **åŠ©æ‰‹æ–‡æœ¬** | **1** | **âœ…** | **æ ¸å¿ƒè®­ç»ƒå†…å®¹ï¼** |
| `<|python_start|>` | 1 | âœ… | å·¥å…·è°ƒç”¨å¼€å§‹ |
| Python ä»£ç  | 1 | âœ… | å­¦ä¹ ä½•æ—¶/å¦‚ä½•è°ƒç”¨å·¥å…· |
| `<|python_end|>` | 1 | âœ… | å·¥å…·è°ƒç”¨ç»“æŸ |
| `<|output_start|>` | 0 | âŒ | å·¥å…·è¾“å‡ºå¼€å§‹ |
| å·¥å…·è¾“å‡º | 0 | âŒ | ç¯å¢ƒè¿”å›ï¼Œä¸éœ€è¦å­¦ä¹  |
| `<|output_end|>` | 0 | âŒ | å·¥å…·è¾“å‡ºç»“æŸ |
| `<|assistant_end|>` | 1 | âœ… | å­¦ä¹ ä½•æ—¶åœæ­¢ |

#### ä»£ç å®ç°

åœ¨ `tokenizer.py` çš„ `render_conversation` æ–¹æ³•ä¸­ï¼š

```python
def render_conversation(self, conversation):
    ids, mask = [], []
    
    def add_tokens(token_ids, mask_val):
        """è¾…åŠ©å‡½æ•°ï¼šæ·»åŠ  token å’Œå¯¹åº”çš„ mask"""
        ids.extend(token_ids)
        mask.extend([mask_val] * len(token_ids))
    
    # å¼€å§‹æ ‡è®°
    add_tokens(bos, 0)  # <|bos|> ä¸è®­ç»ƒ
    
    for message in messages:
        if message["role"] == "user":
            # ç”¨æˆ·æ¶ˆæ¯ï¼šå…¨éƒ¨ä¸è®­ç»ƒ
            add_tokens(user_start, 0)
            add_tokens(user_message_ids, 0)  # â† mask=0
            add_tokens(user_end, 0)
            
        elif message["role"] == "assistant":
            # åŠ©æ‰‹æ¶ˆæ¯ï¼šå¼€å§‹æ ‡è®°ä¸è®­ç»ƒï¼Œå†…å®¹è®­ç»ƒ
            add_tokens(assistant_start, 0)  # â† ä¸è®­ç»ƒå¼€å§‹æ ‡è®°
            
            if isinstance(content, str):
                # çº¯æ–‡æœ¬å›å¤
                add_tokens(content_ids, 1)  # â† mask=1ï¼Œè®­ç»ƒï¼
                
            elif isinstance(content, list):
                # åŒ…å«å·¥å…·è°ƒç”¨çš„å›å¤
                for part in content:
                    if part["type"] == "text":
                        add_tokens(part_ids, 1)  # â† æ–‡æœ¬éƒ¨åˆ†è®­ç»ƒ
                    elif part["type"] == "python":
                        add_tokens(python_start, 1)
                        add_tokens(code_ids, 1)  # â† ä»£ç éƒ¨åˆ†è®­ç»ƒ
                        add_tokens(python_end, 1)
                    elif part["type"] == "python_output":
                        add_tokens(output_start, 0)
                        add_tokens(output_ids, 0)  # â† è¾“å‡ºä¸è®­ç»ƒ
                        add_tokens(output_end, 0)
            
            add_tokens(assistant_end, 1)  # â† ç»“æŸæ ‡è®°è®­ç»ƒï¼ˆå­¦ä¼šåœæ­¢ï¼‰
    
    return ids, mask
```

### 2.3 Mask çš„å®é™…æ•ˆæœï¼ˆå›¾è§£ï¼‰

#### ç¤ºä¾‹ 1ï¼šç®€å•é—®ç­”

```
å¯¹è¯ï¼š
User: "What is 2+2?"
Assistant: "4"

Token åŒ–ï¼š
<|bos|> <|user_start|> What is 2 + 2 ? <|user_end|> <|assistant_start|> 4 <|assistant_end|>
   0          0          0   0  0 0 0 0      0              0                1      1

                                                                              â†‘      â†‘
                                                                        åªè®­ç»ƒè¿™ä¸¤ä¸ªï¼
```

**è®­ç»ƒç›®æ ‡ï¼š**
- ç»™å®š `<|assistant_start|>`ï¼Œé¢„æµ‹ `"4"`
- ç»™å®š `"4"`ï¼Œé¢„æµ‹ `<|assistant_end|>`ï¼ˆå­¦ä¼šåœæ­¢ï¼‰

#### ç¤ºä¾‹ 2ï¼šå¸¦å·¥å…·è°ƒç”¨

```
å¯¹è¯ï¼š
User: "What is 12/60?"
Assistant: <|python_start|>12/60<|python_end|><|output_start|>0.2<|output_end|>The answer is 0.2

Token åŒ–ï¼š
<|bos|> <|user_start|> What is 12/60 ? <|user_end|> 
   0          0          0   0    0   0      0

<|assistant_start|> <|python_start|> 12/60 <|python_end|> 
        0                   1            1          1
                            â†‘            â†‘          â†‘
                        å­¦ä¹ è°ƒç”¨å·¥å…·

<|output_start|> 0.2 <|output_end|> The answer is 0.2 <|assistant_end|>
       0          0         0            1    1     1  1       1
                                         â†‘â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†‘â”€â”€â”€â”€â”€â”€â”€â”€â†‘
                                      å­¦ä¹ å¦‚ä½•ä½¿ç”¨å·¥å…·ç»“æœ
```

**è®­ç»ƒè¦ç‚¹ï¼š**
- âœ… å­¦ä¹ ä½•æ—¶è°ƒç”¨å·¥å…·ï¼ˆ`<|python_start|>`ï¼‰
- âœ… å­¦ä¹ å·¥å…·è°ƒç”¨å†…å®¹ï¼ˆ`12/60`ï¼‰
- âŒ ä¸å­¦ä¹ å·¥å…·è¾“å‡ºï¼ˆ`0.2`ï¼‰â† ç¯å¢ƒå†³å®šçš„
- âœ… å­¦ä¹ å¦‚ä½•å¼•ç”¨å·¥å…·ç»“æœï¼ˆ"The answer is 0.2"ï¼‰

### 2.4 Mask åœ¨æŸå¤±è®¡ç®—ä¸­çš„åº”ç”¨

#### æ•°æ®å‡†å¤‡é˜¶æ®µ

åœ¨ `chat_sft.py` çš„ `sft_data_generator` ä¸­ï¼š

```python
def collate_and_yield(batch):
    nrows = len(batch)
    ncols = max(len(ids) for ids, mask in batch) - 1
    
    # åˆå§‹åŒ–
    inputs = torch.full((nrows, ncols), pad_token_id)
    targets = torch.full((nrows, ncols), -1)  # â† -1 æ˜¯å…³é”®ï¼
    
    for i, (ids, mask) in enumerate(batch):
        n = len(ids)
        ids_tensor = torch.tensor(ids)
        
        # è¾“å…¥å’Œç›®æ ‡
        inputs[i, :n-1] = ids_tensor[:-1]
        targets[i, :n-1] = ids_tensor[1:]
        
        # åº”ç”¨ maskï¼šmask=0 çš„ä½ç½®è®¾ä¸º -1
        mask_tensor = torch.tensor(mask[1:])
        targets[i, :n-1][mask_tensor == 0] = -1  # â† å±è”½
    
    return inputs, targets
```

**å…³é”®ç‚¹ï¼š**
- `targets` ä¸­ mask=0 çš„ä½ç½®è¢«è®¾ä¸º `-1`
- `-1` æ˜¯ PyTorch CrossEntropyLoss çš„ `ignore_index`

#### æŸå¤±è®¡ç®—

```python
# åœ¨æ¨¡å‹çš„ forward æ–¹æ³•ä¸­
def forward(self, inputs, targets):
    logits = self.lm_head(x)  # (B, T, vocab_size)
    
    # äº¤å‰ç†µæŸå¤±ï¼Œè‡ªåŠ¨å¿½ç•¥ target=-1 çš„ä½ç½®
    loss = F.cross_entropy(
        logits.view(-1, vocab_size),
        targets.view(-1),
        ignore_index=-1  # â† å¿½ç•¥ -1
    )
    return loss
```

**å…·ä½“ç¤ºä¾‹ï¼ˆB=2, T=8ï¼‰ï¼š**

```python
# inputs (2, 8)
[[101, 202, 303, 404, 505, 606, 707, 808],
 [909, 1010, 1111, 1212, 1313, 1414, 1515, 1616]]

# targets (2, 8) - æ³¨æ„ -1 çš„ä½ç½®
[[202, 303, 404, -1,  -1,  606, 707, 808],
 [1010, -1,  -1,  -1, 1313, 1414, 1515, -1]]
  â†‘                   â†‘
  è®­ç»ƒ               ä¸è®­ç»ƒ

# è®¡ç®—æŸå¤±æ—¶ï¼š
# - ä½ç½® [0,0]: é¢„æµ‹ 202ï¼Œè®¡ç®—æŸå¤± âœ…
# - ä½ç½® [0,3]: ç›®æ ‡æ˜¯ -1ï¼Œè·³è¿‡ âŒ
# - ä½ç½® [1,1]: ç›®æ ‡æ˜¯ -1ï¼Œè·³è¿‡ âŒ
```

**æ•ˆæœï¼š**
- åªæœ‰ mask=1 çš„ä½ç½®è´¡çŒ®æ¢¯åº¦
- æ¨¡å‹åªå­¦ä¹  Assistant çš„ç”Ÿæˆæ¨¡å¼
- è®­ç»ƒæ•ˆç‡æ›´é«˜ï¼Œæ•ˆæœæ›´å¥½

---

## 3. è®­ç»ƒæµç¨‹è¯¦è§£

### 3.1 å®Œæ•´æ•°æ®æµï¼ˆä»å¯¹è¯åˆ°æ¢¯åº¦ï¼‰

```
æ­¥éª¤ 1: åŠ è½½å¯¹è¯æ•°æ®
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ARC-Easy: 2.3K ç§‘å­¦æ¨ç†              â”‚
â”‚ GSM8K: 8K æ•°å­¦é¢˜                     â”‚
â”‚ SmolTalk: 10K å¯¹è¯                   â”‚
â”‚ Spelling: 600 æ‹¼å†™ä»»åŠ¡               â”‚
â”‚ ...                                  â”‚
â”‚ æ€»è®¡: ~23K é«˜è´¨é‡å¯¹è¯                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
æ­¥éª¤ 2: TaskMixture æ··åˆ
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ‰€æœ‰ä»»åŠ¡æ··åˆæ‰“ä¹±                      â”‚
â”‚ ç¡®ä¿å¤šæ ·æ€§                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
æ­¥éª¤ 3: éå†å¯¹è¯
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ for i in range(ddp_rank, len(ds), world_size): â”‚
â”‚     doc = dataset[i]                 â”‚
â”‚     conversation = {                 â”‚
â”‚         "messages": [...]            â”‚
â”‚     }                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
æ­¥éª¤ 4: Tokenize + Mask
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ids, mask = tokenizer.render_conversation(doc) â”‚
â”‚                                      â”‚
â”‚ ids:  [1, 2, 3, 4, 5, 6, 7, 8, ...]  â”‚
â”‚ mask: [0, 0, 0, 1, 1, 0, 1, 1, ...]  â”‚
â”‚       â†‘     â†‘     â†‘     â†‘            â”‚
â”‚     ä¸è®­ç»ƒ  è®­ç»ƒ ä¸è®­ç»ƒ  è®­ç»ƒ         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
æ­¥éª¤ 5: ç»„ç»‡æ‰¹æ¬¡
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç´¯ç§¯å¤šä¸ªå¯¹è¯                          â”‚
â”‚ æ¯ä¸ªæ‰¹æ¬¡: device_batch_size=4        â”‚
â”‚                                      â”‚
â”‚ inputs:  (4, T)                      â”‚
â”‚ targets: (4, T) â† åŒ…å« -1 çš„å±è”½     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
æ­¥éª¤ 6: æ¨¡å‹å‰å‘ä¼ æ’­
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ logits = model(inputs)               â”‚
â”‚ # (4, T, vocab_size)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
æ­¥éª¤ 7: è®¡ç®—æŸå¤±ï¼ˆå¸¦ maskï¼‰
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ loss = F.cross_entropy(              â”‚
â”‚     logits.view(-1, vocab_size),     â”‚
â”‚     targets.view(-1),                â”‚
â”‚     ignore_index=-1  â† è‡ªåŠ¨å±è”½      â”‚
â”‚ )                                    â”‚
â”‚ # loss: æ ‡é‡                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
æ­¥éª¤ 8: åå‘ä¼ æ’­
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ loss.backward()                      â”‚
â”‚ # åªæœ‰ mask=1 çš„ä½ç½®äº§ç”Ÿæ¢¯åº¦         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
æ­¥éª¤ 9: æ›´æ–°å‚æ•°
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ optimizer.step()                     â”‚
â”‚ model.zero_grad()                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 è¶…å‚æ•°é…ç½®

#### å…³é”®è¶…å‚æ•°

```python
# SFT è¶…å‚æ•°
sft_config = {
    # æ¨¡å‹æ¥æº
    "source": "mid",  # ä» Mid æ¨¡å‹å¼€å§‹
    "model_tag": None,
    "step": None,
    
    # è®­ç»ƒè§„æ¨¡
    "device_batch_size": 4,           # æ¯å¡æ‰¹æ¬¡ï¼ˆå°ï¼‰
    "target_examples_per_step": 32,   # ç›®æ ‡æ‰¹æ¬¡å¤§å°
    "num_epochs": 1,                  # è®­ç»ƒè½®æ•°
    "num_iterations": -1,             # è‡ªåŠ¨è®¡ç®—
    
    # å­¦ä¹ ç‡ï¼ˆå±‚çº§åŒ–ï¼‰
    "unembedding_lr": 0.004,  # è¾“å‡ºå±‚ï¼ˆæœ€å°ï¼‰
    "embedding_lr": 0.2,       # è¾“å…¥å±‚ï¼ˆæœ€å¤§ï¼‰
    "matrix_lr": 0.02,         # ä¸­é—´å±‚
    "weight_decay": 0.0,
    "init_lr_frac": 0.02,      # ä» 2% å¼€å§‹ï¼
    
    # è¯„ä¼°
    "eval_every": 100,
    "eval_steps": 100,
    "eval_metrics_every": 200,
}
```

#### ä¸ Base/Mid çš„å¯¹æ¯”

| è¶…å‚æ•° | Base | Mid | SFT | è¯´æ˜ |
|-------|------|-----|-----|------|
| æ•°æ®é‡ | æ•°åäº¿ | 850K | 23K | è¶Šæ¥è¶Šå°‘ |
| batch_size | 32 | 32 | 4 | SFT æ›´å° |
| å­¦ä¹ ç‡ | 0.02-0.2 | 0.02-0.2 | 0.004-0.2 | ç›¸åŒ |
| init_lr_frac | 1.0 | 1.0 | **0.02** | SFT ä»å¾ˆå°å¼€å§‹ï¼ |
| è®­ç»ƒè½®æ•° | - | 1 | 1 | é€šå¸¸ 1 è½® |
| æ¨¡å‹æ¥æº | éšæœºåˆå§‹åŒ– | Base | **Mid** | ç»§æ‰¿èƒ½åŠ› |

**ä¸ºä»€ä¹ˆ init_lr_frac=0.02ï¼Ÿ**

```python
# å®é™…å­¦ä¹ ç‡
actual_lr = base_lr * init_lr_frac

# ä¾‹å¦‚ï¼š
embedding_lr = 0.2 * 0.02 = 0.004   # å®é™…ä» 0.004 å¼€å§‹
matrix_lr = 0.02 * 0.02 = 0.0004    # å®é™…ä» 0.0004 å¼€å§‹
```

**åŸå› ï¼š**
- Mid æ¨¡å‹å·²ç»å¾ˆå¥½äº†ï¼Œä¸èƒ½ç ´åå·²æœ‰èƒ½åŠ›
- SFT åªæ˜¯"å¾®è°ƒ"ï¼Œä¸æ˜¯é‡æ–°è®­ç»ƒ
- å°å­¦ä¹ ç‡ = è½»æŸ”è°ƒæ•´ = ä¿ç•™åŸæœ‰çŸ¥è¯†

### 3.3 å­¦ä¹ ç‡è°ƒåº¦

```python
def get_lr_multiplier(it):
    # ç®€å•çš„çº¿æ€§è¡°å‡
    lrm = 1.0 - it / num_iterations
    return lrm
```

**å­¦ä¹ ç‡æ›²çº¿ï¼ˆnum_iterations=1000ï¼‰ï¼š**

```
å®é™…å­¦ä¹ ç‡ï¼ˆä»¥ matrix_lr ä¸ºä¾‹ï¼‰
    
    0.0004 |â•²                      â† 0.02 * 0.02 * 1.0
           | â•²
           |  â•²
    0.0002 |   â•²
           |    â•²
    0.0000 |     â•²________________ â† 0.02 * 0.02 * 0.0
           +â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
           0    250   500   750   1000 step
           
åˆå§‹å°±å¾ˆå°ï¼ç„¶åé€æ¸å‡åˆ° 0
```

**å¯¹æ¯” Base Trainingï¼š**

```
Base Training å­¦ä¹ ç‡ï¼š
    
    0.02 |         â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•²  â† å¤§å­¦ä¹ ç‡ï¼Œé•¿æ—¶é—´ä¿æŒ
         |                            â•²
         |                             â•²
    0.01 |                              â•²
         |                               â•²
    0.00 |                                â•â•
         +â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’

SFT å­¦ä¹ ç‡ï¼š
    
  0.0004 |â•²                              â† å°å­¦ä¹ ç‡ï¼Œå¿«é€Ÿè¡°å‡
         | â•²
  0.0002 |  â•²
         |   â•²
  0.0000 |    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
         +â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
```

**è®¾è®¡å“²å­¦ï¼š**
- Base: "ä»é›¶å­¦ä¹ " â†’ éœ€è¦å¤§å­¦ä¹ ç‡
- SFT: "è½»å¾®è°ƒæ•´" â†’ éœ€è¦å°å­¦ä¹ ç‡

### 3.4 è®­ç»ƒå¾ªç¯è¯¦è§£

```python
# é¢„è®¡ç®—è¿­ä»£æ¬¡æ•°
num_iterations = (len(train_ds) // target_examples_per_step) * num_epochs
# ä¾‹å¦‚ï¼š(23000 // 32) * 1 â‰ˆ 718 æ­¥

for step in range(num_iterations):
    last_step = (step == num_iterations)
    
    # ===== è¯„ä¼°éªŒè¯æŸå¤± =====
    if last_step or step % eval_every == 0:
        model.eval()
        val_loader = build_val_loader()
        losses = []
        for _ in range(eval_steps):
            val_inputs, val_targets = next(val_loader)
            with torch.no_grad():
                loss = model(val_inputs, val_targets)
            losses.append(loss)
        val_loss = torch.stack(losses).mean()
        print(f"Step {step} | Val loss: {val_loss:.6f}")
        model.train()
    
    # ===== è¯„ä¼°ä»»åŠ¡æŒ‡æ ‡ =====
    if last_step or (step > 0 and step % eval_metrics_every == 0):
        model.eval()
        # MMLU å‡†ç¡®ç‡
        mmlu_acc = run_chat_eval("MMLU", model, ...)
        # ARC-Easy å‡†ç¡®ç‡
        arc_acc = run_chat_eval("ARC-Easy", model, ...)
        print(f"Step {step} | MMLU: {mmlu_acc:.4f}, ARC: {arc_acc:.4f}")
        model.train()
    
    if last_step:
        break
    
    # ===== è®¡ç®—æ¢¯åº¦ =====
    num_tokens = 0
    for micro_step in range(grad_accum_steps):
        train_inputs, train_targets = next(train_loader)
        
        with autocast_ctx:
            loss = model(train_inputs, train_targets)
        
        loss = loss / grad_accum_steps
        loss.backward()
        
        # ç»Ÿè®¡æœ‰æ•ˆ token æ•°é‡
        num_tokens += (train_targets >= 0).sum()
    
    # ===== æ›´æ–°å­¦ä¹ ç‡ =====
    lrm = get_lr_multiplier(step)
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * lrm
    
    # ===== æ›´æ–°å‚æ•° =====
    for opt in optimizers:
        opt.step()
    model.zero_grad()
    
    # ===== æ—¥å¿— =====
    print(f"Step {step}/{num_iterations} | loss: {loss:.6f} | "
          f"lrm: {lrm:.4f} | num_tokens: {num_tokens}")
```

### 3.5 ä¸€ä¸ªå®Œæ•´ Step çš„ç»´åº¦è¿½è¸ª

å‡è®¾ï¼š`device_batch_size=4, grad_accum_steps=8, max_seq_len=2048`

```python
# ===== Micro-step 1 =====
train_inputs, train_targets = next(train_loader)
# inputs:  (4, 2048) - int32
# targets: (4, 2048) - int64ï¼ŒåŒ…å« -1

# å‰å‘ä¼ æ’­
x = model.transformer.wte(train_inputs)  # (4, 2048, 768)
# ... Transformer blocks ...
logits = model.lm_head(x)  # (4, 2048, 32000)

# æŸå¤±è®¡ç®—
loss = F.cross_entropy(
    logits.view(-1, 32000),   # (8192, 32000)
    train_targets.view(-1),   # (8192,)
    ignore_index=-1
)
# loss: æ ‡é‡ï¼Œä¾‹å¦‚ 2.345

# å½’ä¸€åŒ–å¹¶åå‘ä¼ æ’­
loss = loss / 8  # 2.345 / 8 = 0.293
loss.backward()  # ç´¯ç§¯æ¢¯åº¦

# ===== Micro-step 2-8 =====
# é‡å¤ä¸Šè¿°è¿‡ç¨‹ï¼Œæ¢¯åº¦æŒç»­ç´¯ç§¯

# ===== æ›´æ–°å‚æ•° =====
for opt in optimizers:
    opt.step()  # ä½¿ç”¨ç´¯ç§¯çš„æ¢¯åº¦æ›´æ–°
model.zero_grad()  # æ¸…é›¶ï¼Œå‡†å¤‡ä¸‹ä¸€æ­¥
```

**å…³é”®è§‚å¯Ÿï¼š**
- æ¯ä¸ª micro-step å¤„ç† 4 ä¸ªæ ·æœ¬
- 8 ä¸ª micro-steps = 32 ä¸ªæ ·æœ¬
- åªæœ‰ targets >= 0 çš„ä½ç½®äº§ç”Ÿæ¢¯åº¦
- æ¢¯åº¦ç´¯ç§¯åä¸€æ¬¡æ€§æ›´æ–°

---

## 4. è¯„ä¼°ä½“ç³»

### 4.1 è¯„ä¼°æŒ‡æ ‡æ¦‚è§ˆ

SFT æœ‰ä¸¤ç§è¯„ä¼°æ–¹å¼ï¼š

1. **éªŒè¯æŸå¤± (Validation Loss)** - è¡¡é‡æ¨¡å‹æ‹Ÿåˆåº¦
2. **ä»»åŠ¡å‡†ç¡®ç‡ (Task Accuracy)** - è¡¡é‡å®é™…èƒ½åŠ›

### 4.2 éªŒè¯æŸå¤±è¯„ä¼°

```python
def evaluate_validation_loss():
    model.eval()
    val_loader = build_val_loader()
    losses = []
    
    for _ in range(eval_steps):  # ä¾‹å¦‚ 100 æ­¥
        val_inputs, val_targets = next(val_loader)
        with torch.no_grad():
            loss = model(val_inputs, val_targets)
        losses.append(loss)
    
    val_loss = torch.stack(losses).mean()
    return val_loss
```

**éªŒè¯æ•°æ®æ¥æºï¼š**
```python
val_ds = SmolTalk(split="test")  # 24K æµ‹è¯•å¯¹è¯
```

**Loss çš„æ„ä¹‰ï¼š**
- è¶Šå°è¶Šå¥½
- è¡¡é‡æ¨¡å‹å¯¹å¯¹è¯çš„"å›°æƒ‘åº¦"
- å…¸å‹å€¼ï¼š1.5 - 2.5

### 4.3 ä»»åŠ¡å‡†ç¡®ç‡è¯„ä¼°

#### MMLU è¯„ä¼°

```python
def run_chat_eval(task_name, model, tokenizer, engine, 
                  batch_size=8, max_problems=1024):
    """
    è¯„ä¼°æ¨¡å‹åœ¨é€‰æ‹©é¢˜ä»»åŠ¡ä¸Šçš„è¡¨ç°
    """
    if task_name == "MMLU":
        task = MMLU(subset="all", split="test")
    elif task_name == "ARC-Easy":
        task = ARC(subset="ARC-Easy", split="test")
    
    correct = 0
    total = 0
    
    for idx in range(min(max_problems, len(task))):
        # è·å–é—®é¢˜
        conversation = task[idx]
        # conversation["letters"] = ["A", "B", "C", "D"]
        
        # ç”Ÿæˆå›ç­”
        tokens = tokenizer.render_for_completion(conversation)
        
        with torch.no_grad():
            # é™åˆ¶è¾“å‡ºåªèƒ½æ˜¯ A/B/C/D
            assistant_response = engine.generate_restricted(
                tokens,
                allowed_tokens=conversation["letters"]
            )
        
        # è¯„ä¼°æ­£ç¡®æ€§
        is_correct = task.evaluate(conversation, assistant_response)
        correct += is_correct
        total += 1
    
    accuracy = correct / total
    return accuracy
```

**å…³é”®æŠ€æœ¯ï¼šRestricted Generation**

```python
# é—®é¢˜ï¼šæ¨¡å‹å¯èƒ½ç”Ÿæˆ "The answer is B" è€Œä¸æ˜¯ "B"
# è§£å†³ï¼šå¼ºåˆ¶åªèƒ½è¾“å‡º A/B/C/D ä¸­çš„ä¸€ä¸ª

def generate_restricted(tokens, allowed_tokens):
    # è·å– logits
    logits = model(tokens)[:, -1, :]  # (1, vocab_size)
    
    # æ‰¾åˆ°å…è®¸çš„ token IDs
    allowed_ids = [tokenizer.encode_special(t) for t in allowed_tokens]
    # ä¾‹å¦‚ï¼š[65, 66, 67, 68] å¯¹åº” A, B, C, D
    
    # åªä¿ç•™è¿™äº›ä½ç½®çš„ logits
    restricted_logits = logits[:, allowed_ids]  # (1, 4)
    
    # é€‰æ‹©æœ€å¤§çš„
    best_idx = restricted_logits.argmax()
    return allowed_tokens[best_idx]  # "A" or "B" or "C" or "D"
```

#### ARC è¯„ä¼°

```python
# ARC (AI2 Reasoning Challenge)
# ç±»ä¼¼ MMLUï¼Œä¹Ÿæ˜¯é€‰æ‹©é¢˜

arc_easy_acc = run_chat_eval("ARC-Easy", model, ...)
arc_challenge_acc = run_chat_eval("ARC-Challenge", model, ...)
```

**ç‰¹ç‚¹ï¼š**
- ARC-Easy: è¾ƒç®€å•çš„ç§‘å­¦æ¨ç†
- ARC-Challenge: è¾ƒéš¾çš„ç§‘å­¦æ¨ç†

### 4.4 è¯„ä¼°é¢‘ç‡

```python
# éªŒè¯æŸå¤±ï¼šé¢‘ç¹è¯„ä¼°
eval_every = 100  # æ¯ 100 æ­¥

# ä»»åŠ¡å‡†ç¡®ç‡ï¼šä¸å¤ªé¢‘ç¹
eval_metrics_every = 200  # æ¯ 200 æ­¥

# åŸå› ï¼š
# - éªŒè¯æŸå¤±å¿«é€Ÿï¼ˆåªæ˜¯å‰å‘ä¼ æ’­ï¼‰
# - ä»»åŠ¡å‡†ç¡®ç‡æ…¢ï¼ˆéœ€è¦ç”Ÿæˆ + è§£æï¼‰
```

### 4.5 è®­ç»ƒæ—¥å¿—ç¤ºä¾‹

```
Step 00000 | Validation loss: 2.456789
Step 00000 | mmlu_acc: 0.2543, arc_easy_acc: 0.4567

Step 00100/00718 | Training loss: 2.123456 | lrm: 0.8607 | num_tokens: 52,341
Step 00100 | Validation loss: 2.234567

Step 00200/00718 | Training loss: 1.987654 | lrm: 0.7215 | num_tokens: 51,892
Step 00200 | Validation loss: 2.098765
Step 00200 | mmlu_acc: 0.3245, arc_easy_acc: 0.5123  â† æå‡äº†ï¼

...

Step 00700/00718 | Training loss: 1.765432 | lrm: 0.0251 | num_tokens: 50,234
Step 00718 | Validation loss: 1.823456
Step 00718 | mmlu_acc: 0.4567, arc_easy_acc: 0.6234  â† æ˜¾è‘—æå‡ï¼
```

**è§‚å¯Ÿï¼š**
- Training loss æŒç»­ä¸‹é™
- ä»»åŠ¡å‡†ç¡®ç‡ç¨³æ­¥æå‡
- å­¦ä¹ ç‡é€æ¸å‡å°

---

## 5. ä¸å‰åºé˜¶æ®µçš„å¯¹æ¯”

### 5.1 å››é˜¶æ®µå…¨å¯¹æ¯”

| ç»´åº¦ | Base | Mid | SFT | RL |
|-----|------|-----|-----|-----|
| **æ•°æ®é‡** | æ•°åäº¿ tokens | 850K å¯¹è¯ | 23K å¯¹è¯ | 8K å¯¹è¯ |
| **æ•°æ®æº** | ç½‘é¡µæŠ“å– | å…¬å¼€æ•°æ®é›† | ç²¾é€‰å¯¹è¯ | ç¯å¢ƒåé¦ˆ |
| **æ•°æ®è´¨é‡** | ä½ | ä¸­ | é«˜ | é«˜ |
| **æ¨¡å‹æ¥æº** | éšæœºåˆå§‹åŒ– | Base | Mid | SFT |
| **è®­ç»ƒç›®æ ‡** | è¯­è¨€å»ºæ¨¡ | ä»»åŠ¡èƒ½åŠ› | æŒ‡ä»¤éµå¾ª | åå¥½å¯¹é½ |
| **å­¦ä¹ ç‡** | å¤§ (0.02-0.2) | ä¸­ (0.02-0.2) | å° (0.0004-0.004) | å° (0.0008-0.01) |
| **Batch Size** | å¤§ (32) | å¤§ (32) | å° (4) | ä¸­ (8) |
| **è®­ç»ƒæ—¶é•¿** | æ•°å¤©-æ•°å‘¨ | æ•°å°æ—¶-1å¤© | æ•°å°æ—¶ | æ•°å°æ—¶ |
| **Loss ç±»å‹** | å…¨éƒ¨ token | å…¨éƒ¨ token | **Masked** | **Policy Gradient** |
| **è¯„ä¼°æ–¹å¼** | BPB | BPB | å‡†ç¡®ç‡ | Pass@k |

### 5.2 æ•°æ®æ ¼å¼æ¼”å˜

#### Base Training

```
çº¯æ–‡æœ¬æµï¼š
"The quick brown fox jumps over the lazy dog. ..."

Token åŒ–ï¼š
[15496, 995, 831, 374, 264, 1296, ...]

è®­ç»ƒï¼šå…¨éƒ¨ token
```

#### Mid Training

```
ç»“æ„åŒ–å¯¹è¯ï¼ˆä½†ä»ç„¶è®­ç»ƒæ‰€æœ‰å†…å®¹ï¼‰ï¼š
{
    "messages": [
        {"role": "user", "content": "..."},
        {"role": "assistant", "content": "..."}
    ]
}

Token åŒ–ï¼š
[<|bos|>, <|user_start|>, ..., <|assistant_start|>, ...]

è®­ç»ƒï¼šå…¨éƒ¨ tokenï¼ˆåŒ…æ‹¬ user éƒ¨åˆ†ï¼‰
```

#### SFT

```
ç»“æ„åŒ–å¯¹è¯ + Maskï¼š
{
    "messages": [
        {"role": "user", "content": "..."},
        {"role": "assistant", "content": "..."}
    ]
}

Token åŒ– + Maskï¼š
ids:  [<|bos|>, <|user_start|>, ..., <|assistant_start|>, ...]
mask: [0,       0,              ..., 0,                   ...]
      [                             1, 1, 1, ...]
      â†‘                             â†‘
    ä¸è®­ç»ƒ                         åªè®­ç»ƒè¿™éƒ¨åˆ†ï¼
```

**å…³é”®åˆ›æ–°ï¼šMask æœºåˆ¶ï¼**

### 5.3 Loss è®¡ç®—æ–¹å¼å¯¹æ¯”

```python
# ===== Base/Mid Training =====
loss = F.cross_entropy(
    logits.view(-1, vocab_size),
    targets.view(-1)
    # æ‰€æœ‰ä½ç½®éƒ½è®¡ç®—æŸå¤±
)

# ===== SFT =====
loss = F.cross_entropy(
    logits.view(-1, vocab_size),
    targets.view(-1),
    ignore_index=-1  # â† å…³é”®å·®å¼‚ï¼
)
# targets ä¸­ mask=0 çš„ä½ç½®æ˜¯ -1ï¼Œè¢«å¿½ç•¥
```

### 5.4 æ¨¡å‹èƒ½åŠ›æ¼”å˜

**æµ‹è¯•å¯¹è¯ï¼š**

```
User: "Write a Python function to add two numbers."
```

#### Base æ¨¡å‹è¾“å‡º

```
def add(a, b):
    return a + b

def subtract(a, b):
    return a - b

def multiply(a, b):
    return a * b
...
```

**é—®é¢˜ï¼š**
- âŒ ç»§ç»­ç”Ÿæˆæ— å…³å‡½æ•°
- âŒ ä¸çŸ¥é“ä½•æ—¶åœæ­¢
- âŒ æ²¡æœ‰éµå¾ª"åªè¦åŠ æ³•"çš„æŒ‡ä»¤

#### Mid æ¨¡å‹è¾“å‡º

```
<|assistant_start|>def add(a, b):
    return a + b<|assistant_end|>

<|user_start|>Can you also write subtract?<|user_end|>
...
```

**é—®é¢˜ï¼š**
- âœ… çŸ¥é“å¯¹è¯æ ¼å¼
- âœ… èƒ½ç”Ÿæˆæ­£ç¡®ä»£ç 
- âŒ ç»§ç»­ç”Ÿæˆè™šæ„çš„ç”¨æˆ·æ¶ˆæ¯
- âŒ è§’è‰²æ··æ·†

#### SFT æ¨¡å‹è¾“å‡º

```
Here's a Python function to add two numbers:

```python
def add(a, b):
    return a + b
```

This function takes two parameters and returns their sum.
```

**ä¼˜ç‚¹ï¼š**
- âœ… ç›´æ¥å›ç­”é—®é¢˜
- âœ… æ ¼å¼è§„èŒƒï¼ˆMarkdownï¼‰
- âœ… ç®€æ´æ˜äº†
- âœ… é€‚æ—¶åœæ­¢
- âœ… æä¾›è¯´æ˜

### 5.5 è®­ç»ƒæ•ˆç‡å¯¹æ¯”

**å‚æ•°æ›´æ–°æ¬¡æ•°ï¼š**

å‡è®¾ 200M å‚æ•°çš„æ¨¡å‹ï¼š

| é˜¶æ®µ | æ•°æ®é‡ | æ­¥æ•° | æ¯ä¸ªå‚æ•°çœ‹åˆ°çš„æ•°æ® |
|-----|--------|------|-------------------|
| Base | 10B tokens | 50K | 50K æ¬¡æ›´æ–° |
| Mid | 850K å¯¹è¯ | 2K | 2K æ¬¡æ›´æ–° |
| SFT | 23K å¯¹è¯ | 718 | **718 æ¬¡æ›´æ–°** |

**è®­ç»ƒæ—¶é—´ï¼ˆ8 å¡ A100ï¼‰ï¼š**

| é˜¶æ®µ | è®­ç»ƒæ—¶é—´ | æ¯æ­¥æ—¶é—´ |
|-----|---------|---------|
| Base | 3 å¤© | ~5 ç§’ |
| Mid | 6 å°æ—¶ | ~10 ç§’ |
| SFT | **2 å°æ—¶** | ~10 ç§’ |

**ä¸ºä»€ä¹ˆ SFT è¿™ä¹ˆå¿«ï¼Ÿ**
- æ•°æ®é‡å°ï¼ˆ23K vs 850Kï¼‰
- è½®æ•°å°‘ï¼ˆ1 epochï¼‰
- åªéœ€è¦"å¾®è°ƒ"ï¼Œä¸éœ€è¦"é‡è®­ç»ƒ"

---

## 6. å®æˆ˜æ¡ˆä¾‹åˆ†æ

### 6.1 å®Œæ•´è®­ç»ƒæµç¨‹

#### é˜¶æ®µ 1: Base Training (å·²å®Œæˆ)

```bash
# è¾“å‡ºï¼šbase_checkpoints/d12/step_00050000.pt
```

#### é˜¶æ®µ 2: Mid Training (å·²å®Œæˆ)

```bash
# è¾“å‡ºï¼šmid_checkpoints/d12/step_XXXXX.pt
```

#### é˜¶æ®µ 3: SFT (å½“å‰é˜¶æ®µ)

```bash
# å• GPU è°ƒè¯•
python -m scripts.chat_sft

# 8 GPU è®­ç»ƒ
torchrun --standalone --nproc_per_node=8 -m scripts.chat_sft \
    --source=mid \
    --device_batch_size=4 \
    --num_epochs=1 \
    --run=sft_d12

# è¾“å‡ºï¼šchatsft_checkpoints/d12/step_00718.pt
```

### 6.2 è®­ç»ƒæ•°æ®ç¤ºä¾‹

#### ç¤ºä¾‹ 1: ARC-Easy

```json
{
    "messages": [
        {
            "role": "user",
            "content": "Which material is the best conductor of electricity?\nA. wood\nB. plastic\nC. copper\nD. rubber"
        },
        {
            "role": "assistant",
            "content": "C"
        }
    ],
    "letters": ["A", "B", "C", "D"]
}
```

**Tokenize åï¼š**
```
<|bos|> <|user_start|> Which material ... <|user_end|> 
<|assistant_start|> C <|assistant_end|>

Mask:
[0,     0,             0,    0,    ..., 0,
 0,                    1, 1]
                       â†‘  â†‘
                  åªè®­ç»ƒ "C" å’Œç»“æŸæ ‡è®°
```

#### ç¤ºä¾‹ 2: GSM8K

```json
{
    "messages": [
        {
            "role": "user",
            "content": "If 5 apples cost $10, how much do 3 apples cost?"
        },
        {
            "role": "assistant",
            "content": [
                {"type": "text", "text": "First, find the cost per apple:\n"},
                {"type": "python", "text": "10/5"},
                {"type": "python_output", "text": "2"},
                {"type": "text", "text": "\nSo each apple costs $2. For 3 apples:\n"},
                {"type": "python", "text": "2*3"},
                {"type": "python_output", "text": "6"},
                {"type": "text", "text": "\n#### 6"}
            ]
        }
    ]
}
```

**Mask è§„åˆ™ï¼š**
```
<|user_start|> ... <|user_end|>                           [mask=0]
<|assistant_start|>                                        [mask=0]
    First, find the cost per apple:                        [mask=1] âœ…
    <|python_start|> 10/5 <|python_end|>                  [mask=1] âœ…
    <|output_start|> 2 <|output_end|>                     [mask=0] âŒ
    So each apple costs $2. For 3 apples:                 [mask=1] âœ…
    <|python_start|> 2*3 <|python_end|>                   [mask=1] âœ…
    <|output_start|> 6 <|output_end|>                     [mask=0] âŒ
    #### 6                                                  [mask=1] âœ…
<|assistant_end|>                                          [mask=1] âœ…
```

### 6.3 è®­ç»ƒè¿‡ç¨‹ç›‘æ§

#### åˆå§‹çŠ¶æ€ (Step 0)

```
Step 00000 | Validation loss: 2.456
Step 00000 | mmlu_acc: 0.254, arc_easy_acc: 0.457
```

**è§£è¯»ï¼š**
- Val loss ~2.5ï¼šMid æ¨¡å‹çš„èµ·ç‚¹
- MMLU 25.4%ï¼šæ¯”éšæœºçŒœæµ‹ (25%) ç•¥å¥½
- ARC 45.7%ï¼šè¿˜å¯ä»¥

#### è®­ç»ƒä¸­æœŸ (Step 350)

```
Step 00350/00718 | Training loss: 1.876 | lrm: 0.5125
Step 00350 | Validation loss: 2.012
Step 00400 | mmlu_acc: 0.367, arc_easy_acc: 0.589
```

**è§£è¯»ï¼š**
- Training loss ä¸‹é™åˆ° 1.87
- Val loss ä¸‹é™åˆ° 2.01
- MMLU æå‡åˆ° 36.7% (+11.3%)
- ARC æå‡åˆ° 58.9% (+13.2%)

#### è®­ç»ƒå®Œæˆ (Step 718)

```
Step 00718/00718 | Training loss: 1.654 | lrm: 0.0000
Step 00718 | Validation loss: 1.823
Step 00718 | mmlu_acc: 0.423, arc_easy_acc: 0.645
```

**æœ€ç»ˆæ”¶è·ï¼š**
- Val loss: 2.456 â†’ 1.823 (-25.8%)
- MMLU: 25.4% â†’ 42.3% (+16.9%)
- ARC: 45.7% â†’ 64.5% (+18.8%)

### 6.4 æ¨¡å‹å¯¹æ¯”æµ‹è¯•

#### æµ‹è¯• 1: ç®€å•é—®ç­”

```
è¾“å…¥: "What is the capital of France?"
```

| æ¨¡å‹ | è¾“å‡º | è¯„åˆ† |
|-----|------|------|
| Base | "Paris is capital France city..." | â­â­ |
| Mid | "The capital of France is Paris, which is..." | â­â­â­ |
| SFT | "Paris." | â­â­â­â­â­ |

**SFT çš„ä¼˜åŠ¿ï¼š** ç®€æ´ã€ç›´æ¥ã€å‡†ç¡®

#### æµ‹è¯• 2: æŒ‡ä»¤éµå¾ª

```
è¾“å…¥: "List three colors in JSON format."
```

| æ¨¡å‹ | è¾“å‡º | è¯„åˆ† |
|-----|------|------|
| Mid | "Red, blue, green are three colors." | âŒ |
| SFT | `{"colors": ["red", "blue", "green"]}` | âœ… |

**SFT çš„ä¼˜åŠ¿ï¼š** ä¸¥æ ¼éµå¾ªæ ¼å¼è¦æ±‚

#### æµ‹è¯• 3: å·¥å…·ä½¿ç”¨

```
è¾“å…¥: "What is 123 * 456?"
```

| æ¨¡å‹ | è¾“å‡º | è¯„åˆ† |
|-----|------|------|
| Mid | "123 * 456 = <|python_start|>123*456<|python_end|>..." ç„¶åç»§ç»­ç”Ÿæˆ | â­â­â­ |
| SFT | "<|python_start|>123*456<|python_end|><|output_start|>56088<|output_end|>The answer is 56,088." | â­â­â­â­â­ |

**SFT çš„ä¼˜åŠ¿ï¼š** 
- æ­£ç¡®ä½¿ç”¨å·¥å…·
- ç­‰å¾…å·¥å…·è¾“å‡º
- æ•´åˆç»“æœç»™å‡ºç­”æ¡ˆ
- é€‚æ—¶åœæ­¢

---

## 7. å…³é”®è¦ç‚¹æ€»ç»“

### 7.1 SFT çš„æœ¬è´¨

```
SFT = Mid Model + High-Quality Demonstrations + Mask
    = èƒ½åŠ›æ¨¡å‹ + è¡Œä¸ºç¤ºèŒƒ + ç²¾ç¡®è®­ç»ƒ
    = åº”ç”¨å°±ç»ªçš„å¯¹è¯åŠ©æ‰‹
```

### 7.2 Mask çš„é‡è¦æ€§

**æ²¡æœ‰ Maskï¼š**
```
æ¨¡å‹å­¦ä¹ ï¼šç”¨æˆ·æ€ä¹ˆæé—® + åŠ©æ‰‹æ€ä¹ˆå›ç­”
é—®é¢˜ï¼šæµªè´¹èµ„æºï¼Œå¯èƒ½è§’è‰²æ··æ·†
```

**æœ‰ Maskï¼š**
```
æ¨¡å‹åªå­¦ä¹ ï¼šåŠ©æ‰‹æ€ä¹ˆå›ç­”
å¥½å¤„ï¼šé«˜æ•ˆã€ç²¾å‡†ã€æ•ˆæœå¥½
```

### 7.3 SFT vs RL

| ç»´åº¦ | SFT | RL |
|-----|-----|-----|
| è®­ç»ƒæ–¹å¼ | ç›‘ç£å­¦ä¹  | å¼ºåŒ–å­¦ä¹  |
| éœ€è¦ä»€ä¹ˆ | æ­£ç¡®ç­”æ¡ˆ | å¥–åŠ±å‡½æ•° |
| å­¦ä¹ å†…å®¹ | æ¨¡ä»¿ç¤ºä¾‹ | æ¢ç´¢ä¼˜åŒ– |
| é€‚ç”¨åœºæ™¯ | æ ¼å¼ã€ç¤¼ä»ª | æ¨ç†ã€åå¥½ |
| è®­ç»ƒéš¾åº¦ | ç®€å• | å¤æ‚ |

### 7.4 å®è·µå»ºè®®

**æ•°æ®å‡†å¤‡ï¼š**
- âœ… ä¼˜å…ˆè´¨é‡ï¼Œä¸æ˜¯æ•°é‡
- âœ… ç¡®ä¿ç¤ºä¾‹å±•ç¤ºæœ€ä½³å®è·µ
- âœ… è¦†ç›–ç›®æ ‡åº”ç”¨åœºæ™¯
- âœ… åŒ…å«å¤šæ ·åŒ–çš„ä»»åŠ¡ç±»å‹

**è®­ç»ƒé…ç½®ï¼š**
- å­¦ä¹ ç‡ï¼šä»å¾ˆå°å¼€å§‹ (init_lr_frac=0.02)
- è®­ç»ƒè½®æ•°ï¼šé€šå¸¸ 1 è½®å°±å¤Ÿ
- Batch sizeï¼šå¯ä»¥æ¯”è¾ƒå° (4-8)
- è¯„ä¼°ï¼šé¢‘ç¹æ£€æŸ¥å‡†ç¡®ç‡

**å¸¸è§é”™è¯¯ï¼š**
- âŒ å­¦ä¹ ç‡å¤ªå¤§ â†’ ç ´åå·²æœ‰èƒ½åŠ›
- âŒ è®­ç»ƒå¤ªä¹… â†’ è¿‡æ‹Ÿåˆ
- âŒ å¿˜è®° Mask â†’ æ•ˆæœå·®
- âŒ æ•°æ®è´¨é‡ä½ â†’ ç™½è´¹åŠ›æ°”

---

## é™„å½•

### A. å®Œæ•´è®­ç»ƒå‘½ä»¤

```bash
# ä» Mid æ¨¡å‹å¼€å§‹
python -m scripts.chat_sft \
    --source=mid \
    --device_batch_size=4 \
    --target_examples_per_step=32 \
    --num_epochs=1 \
    --eval_every=100 \
    --run=sft_experiment

# ä» Base æ¨¡å‹å¼€å§‹ï¼ˆä¸æ¨èï¼‰
python -m scripts.chat_sft \
    --source=base \
    --model_tag=d12 \
    --step=50000 \
    --device_batch_size=4

# å¤š GPU
torchrun --standalone --nproc_per_node=8 -m scripts.chat_sft \
    --device_batch_size=4 \
    --run=sft_production
```

### B. æ•°æ®ç»Ÿè®¡

| æ•°æ®é›† | è®­ç»ƒé‡ | æµ‹è¯•é‡ | ä»»åŠ¡ç±»å‹ |
|-------|-------|-------|---------|
| ARC-Easy | 2.3K | 570 | ç§‘å­¦æ¨ç†ï¼ˆç®€å•ï¼‰ |
| ARC-Challenge | 1.1K | 299 | ç§‘å­¦æ¨ç†ï¼ˆå›°éš¾ï¼‰ |
| GSM8K | 7.5K | 1.3K | æ•°å­¦æ¨ç† |
| SmolTalk | 10K | 24K | é€šç”¨å¯¹è¯ |
| Identity | 1K | - | èº«ä»½è®¾å®š |
| SimpleSpelling | 300 | - | æ‹¼å†™åŸºç¡€ |
| SpellingBee | 300 | - | å­—æ¯è®¡æ•° |
| **æ€»è®¡** | **~23K** | **~26K** | - |

### C. å¸¸è§é—®é¢˜

**Q1: SFT å¯ä»¥è·³è¿‡å—ï¼Ÿ**
- ç†è®ºä¸Šå¯ä»¥ï¼Œä½†ä¸æ¨è
- æ²¡æœ‰ SFTï¼Œæ¨¡å‹ä¸ä¼šéµå¾ªæŒ‡ä»¤

**Q2: èƒ½ä¸èƒ½å¤šåšå‡ è½® SFTï¼Ÿ**
- å¯ä»¥ï¼Œä½†é€šå¸¸ 1 è½®å°±å¤Ÿ
- å¤šè½®å¯èƒ½è¿‡æ‹Ÿåˆ
- æ›´å¥½çš„é€‰æ‹©æ˜¯å¢åŠ æ•°æ®å¤šæ ·æ€§

**Q3: ä¸ºä»€ä¹ˆä¸ç›´æ¥ä» Base åš SFTï¼Ÿ**
- å¯ä»¥ï¼Œä½†æ•ˆæœä¸å¦‚ Baseâ†’Midâ†’SFT
- Mid Training æä¾›äº†ä»»åŠ¡ç†è§£èƒ½åŠ›
- SFT åªéœ€è¦è°ƒæ•´è¡Œä¸ºï¼Œä¸éœ€è¦å­¦ä¹ èƒ½åŠ›

**Q4: SFT æ•°æ®æ€ä¹ˆå‡†å¤‡ï¼Ÿ**
- äººå·¥ç¼–å†™é«˜è´¨é‡å¯¹è¯
- ä»çœŸå®ç”¨æˆ·äº¤äº’ä¸­ç­›é€‰
- ä½¿ç”¨æ›´å¼ºæ¨¡å‹ç”Ÿæˆï¼ˆDistillationï¼‰
- äººå·¥å®¡æ ¸å’Œä¿®æ­£

**Q5: Mask æ˜¯å¿…é¡»çš„å—ï¼Ÿ**
- å¼ºçƒˆæ¨èï¼
- æ²¡æœ‰ Mask ä¹Ÿèƒ½è®­ç»ƒï¼Œä½†æ•ˆæœå·®
- Mask è®©è®­ç»ƒæ›´é«˜æ•ˆã€æ›´ç²¾å‡†

---

## æ€»ç»“

**SFT çš„ä¸‰ä¸ªå…³é”®åˆ›æ–°ï¼š**

1. **Mask æœºåˆ¶** - åªè®­ç»ƒ Assistant çš„å›å¤
2. **å°å­¦ä¹ ç‡** - ä¿æŠ¤å·²æœ‰èƒ½åŠ›ï¼Œè½»æŸ”è°ƒæ•´
3. **é«˜è´¨é‡æ•°æ®** - å°‘è€Œç²¾ï¼Œå±•ç¤ºæœ€ä½³å®è·µ

**è®­ç»ƒæµç¨‹ï¼š**
```
Mid Model (ä»»åŠ¡èƒ½åŠ›)
    â†“
+ High-Quality Demonstrations (è¡Œä¸ºç¤ºèŒƒ)
    â†“
+ Mask Mechanism (ç²¾ç¡®è®­ç»ƒ)
    â†“
SFT Model (åº”ç”¨å°±ç»ª)
    â†“
å‡†å¤‡å¥½æœåŠ¡ç”¨æˆ·ï¼
```

**ä¸‹ä¸€æ­¥ï¼šRL**
- é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥ä¼˜åŒ–
- å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡
- å¯¹é½äººç±»åå¥½

---

*æœ¬æ–‡æ¡£åŸºäº nanochat é¡¹ç›®åˆ†æç”Ÿæˆ*  
*é€‚åˆ LLM åˆå­¦è€…ç†è§£ SFT çš„å®Œæ•´æµç¨‹*  
*åˆ›å»ºæ—¶é—´: 2025å¹´12æœˆ21æ—¥*
